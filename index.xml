<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tom Beer</title>
    <link>https://tom-beer.github.io/</link>
      <atom:link href="https://tom-beer.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Tom Beer</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 18 Jun 2023 03:07:42 +0300</lastBuildDate>
    <image>
      <url>https://tom-beer.github.io/images/icon_huce93ec8a6fd7e12c2beec32639fe78a0_7050_512x512_fill_lanczos_center_2.png</url>
      <title>Tom Beer</title>
      <link>https://tom-beer.github.io/</link>
    </image>
    
    <item>
      <title>Hotel Recommender - WIP</title>
      <link>https://tom-beer.github.io/post/hotel_recommender/</link>
      <pubDate>Sun, 18 Jun 2023 03:07:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/hotel_recommender/</guid>
      <description>&lt;script
	type=&#34;module&#34;
	src=&#34;https://gradio.s3-us-west-2.amazonaws.com/3.34.0/gradio.js&#34;
&gt;&lt;/script&gt;
&lt;p&gt;&lt;gradio-app src=&#34;https://tom-beer-hotel-recommender.hf.space&#34;&gt;&lt;/gradio-app&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting recipe popularity</title>
      <link>https://tom-beer.github.io/post/food_blog_popularity/</link>
      <pubDate>Mon, 31 Oct 2022 03:07:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/food_blog_popularity/</guid>
      <description>&lt;h3 id=&#34;my-favorite-food-blog&#34;&gt;My favorite food blog&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;m an avid reader of a multitude of food blogs.
While I wish I read books, I just can&amp;rsquo;t prioritize it high enough, and apart from professional material, the only reading I do is food blogs.
I love 
&lt;a href=&#34;https://www.davidlebovitz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Lebovitz&lt;/a&gt; for the french drinks and apéros, and 
&lt;a href=&#34;https://greenkitchenstories.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Green Kitchen Stories&lt;/a&gt; for healthy, kid friendly (Yaar loves 
&lt;a href=&#34;https://greenkitchenstories.com/apple-almond-buckwheat-muffins/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these muffins&lt;/a&gt;) recipes.
In Hebrew, I check 
&lt;a href=&#34;https://www.ptitim.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ptitim&lt;/a&gt;, 
&lt;a href=&#34;https://www.limortiroche.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Limor Tiroche&lt;/a&gt; and 
&lt;a href=&#34;https://www.michalwaxman.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michal Waxman&lt;/a&gt; on a weekly basis and read every word in every post.&lt;/p&gt;
&lt;p&gt;But above all these, my favorite food blog is 
&lt;a href=&#34;https://smittenkitchen.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Smitten Kitchen&lt;/a&gt;.
My metric here (we&amp;rsquo;re in a quantitative field after all) is the total number of recipes made.
This proxy may be skewed by ultra-recurrent recipes
(like the 
&lt;a href=&#34;https://smittenkitchen.com/2010/05/oatmeal-pancakes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;oatmeal pancakes&lt;/a&gt; I make every Saturday morning for the last ~18 months),
but it&amp;rsquo;s still a useful metric.&lt;/p&gt;
&lt;p&gt;Having wanted to play around with web scraping and parsing,
I thought it would be a fun weekend project to build a popularity prediction model for Smitten Kitchen recipes.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;Beautiful Soup helped with basic parsing,
on top of which specific features like the publish date, categories, post title, introduction and the recipe itself were extracted.
The number of comments was defined as the measure of recipe popularity (more on this decision in the last section).&lt;br&gt;
The resulting dataset (not showing the category column) has 505 recipes and looks like this:
&lt;img src=&#34;df2.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As can be expected, the number of comments is heavy tailed.
But this tail is rather fat - the standard deviation is almost as large as the median:
&lt;img src=&#34;n_comments_hist.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;setting-the-baseline&#34;&gt;Setting the baseline&lt;/h3&gt;
&lt;p&gt;RMSE on the test set (~100 recipes) was chosen as the evaluation criteria for all models.&lt;/p&gt;
&lt;p&gt;A useful baseline for regression tasks is simply to predict the target&amp;rsquo;s mean.
This yields an RMSE of &lt;strong&gt;202&lt;/strong&gt;, a bit higher than the target&amp;rsquo;s STD (due to train/test sample variability).&lt;/p&gt;
&lt;p&gt;It couldn&amp;rsquo;t be hard to beat this error, right? Let&amp;rsquo;s see..&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;recipe-categories&#34;&gt;Recipe Categories&lt;/h3&gt;
&lt;p&gt;Each recipe has a few categories (tags) - an average of 5 per blog entry.
The most prevalent are &lt;em&gt;vegetarian, summer, breakfast, cake, salad&lt;/em&gt; and &lt;em&gt;weeknight favorite&lt;/em&gt;.
The 184 categories were encoded as bits and passed to a CatBoost model with default parameters.
The RMSE decreased to &lt;strong&gt;194&lt;/strong&gt; - quite underwhelming.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-direction-of-time&#34;&gt;The Direction of Time&lt;/h3&gt;
&lt;p&gt;Time is a significant predictor of popularity, especially in the early days of the site.&lt;/p&gt;
&lt;img src=&#34;time.png&#34;&gt;
&lt;p&gt;Using the annual means from the train set as an estimator for recipe popularity yields an
RMSE of &lt;strong&gt;181&lt;/strong&gt; - now we&amp;rsquo;re getting somewhere!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;incorporating-free-text&#34;&gt;Incorporating free text&lt;/h3&gt;
&lt;p&gt;The dataset includes three fields of free text: post title, an introduction and the recipe itself.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.sbert.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sentence Transformers&lt;/a&gt; provides a convenient API for encoding chunks of text,
but they don&amp;rsquo;t play nicely with scikit-learn pipelines. Recently I came across a compact library called

&lt;a href=&#34;https://github.com/koaning/embetter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Embetter&lt;/em&gt;&lt;/a&gt; which does just that - a scikit-learn compatible interface
for sentence transformers.
It also has an interface over 
&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;timm&lt;/a&gt; for image use cases.
With Embetter, predicting a target from text is as easy as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = make_pipeline(ColumnGrabber(&amp;quot;title&amp;quot;), SentenceEncoder(&#39;all-MiniLM-L6-v2&#39;), RandomForestRegressor())
pipe.fit(df_train, df_train.n_comments)
mean_squared_error(df_val.n_comments, pipe.predict(df_val), squared=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After some experiments, I settled on the following pipeline:&lt;/p&gt;
&lt;img src=&#34;pipe.png&#34; width=600 height=600&gt;
&lt;p&gt;Embedding the other text fields did not add predictive power.
Applying PCA and using other regressors just made things worse.
The RMSE for this setup was &lt;strong&gt;174&lt;/strong&gt; - only slightly better than using the year alone. Meh.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;closing-thoughts&#34;&gt;Closing thoughts&lt;/h3&gt;
&lt;p&gt;While this was a lot of fun, I can&amp;rsquo;t help but feel a bit disappointed in failing to predict recipe popularity.
The thing is, I equated popularity with the number of comments.
But it&amp;rsquo;s a mere proxy for popularity. And as we&amp;rsquo;ve seen in a

&lt;a href=&#34;https://tom-beer.github.io/post/proxies_ml/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;,
this has implications for the feasibility and usability of the models built upon it.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;takeaways&#34;&gt;Takeaways&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://smittenkitchen.com/2019/06/chocolate-budino/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This is a fabulous dessert&lt;/a&gt;. Go make it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Revisiting the bias variance tradeoff</title>
      <link>https://tom-beer.github.io/post/revisiting_bias_variance/</link>
      <pubDate>Mon, 25 Jul 2022 10:13:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/revisiting_bias_variance/</guid>
      <description>&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;A while ago I was tasked with developing a model predicting whether patients would be referred to the emergency room,
based on their medical presentation, demographics, etc.
The random forest was performing much better on the train set than on the test set.
Technically, it&amp;rsquo;s a case of overfitting, but there was a disagreement among my peers if this was a bad thing or not.
After all, it was better than models that did not exhibit a train-test gap (e.g. logistic regression).
But still I was not able to convince that test performance is all that should matter.
So I had a little literature review, and presented it in our weekly journal club.
This post is a summary of that presentation.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;Every intro to ML course or textbook teaches us the bias variance tradeoff as universal truth -
that test error curve follows a U-shape with respect to model complexity.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Quote and figure from 
&lt;a href=&#34;https://hastie.su.domains/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISLRv2, 2021&lt;/a&gt;&lt;/p&gt;
&lt;img src=&#34;image5.png&#34; width=600 height=600 align=&#34;middle&#34;&gt;
&lt;p&gt;Yet everywhere around us we see evidence that “bigger models are always better”
&lt;sup&gt;
&lt;a href=&#34;https://microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1&lt;/a&gt;,

&lt;a href=&#34;http://proceedings.mlr.press/v97/tan19a/tan19a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;img src=&#34;image4.png&#34; width=1400 height=1400 align=&#34;middle&#34;&gt;
&lt;p&gt;How do we relieve this tension? Is it just the difference between traditional machine learning and deep learning?
Is the tradeoff real at all? Let’s take a stab at settling this.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-defense&#34;&gt;The Defense&lt;/h3&gt;
&lt;p&gt;On the one hand, there is empirical grounding for the tradeoff - it can be shown to exist in models like KNN, polynomial regression, kernel regression, decision trees and more.&lt;/p&gt;
&lt;p&gt;There is some theoretical justification for it too. Generalization bounds that grow with complexity (VC/Rademacher) are interpreted as if simple models are preferable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“In real applications, learning models with lower VC(H) tend to generalize better”, Learning From Data, 2021&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;image6.png&#34; width=1400 height=1400 align=&#34;middle&#34;&gt;
&lt;img src=&#34;image8.png&#34; width=1400 height=1400 align=&#34;middle&#34;&gt;
&lt;p&gt;As an ML history fun fact, the tradeoff was not popularized until the 1992 paper
“Neural Networks and the Bias/Variance Dilemma” by Geman et. al.
This same paper also introduced the &lt;strong&gt;decomposition&lt;/strong&gt; of the MSE to bias and variance components.
As the decomposition is usually discussed in the same context, it adds to the strong intuition of the tradeoff (even though it does not imply it).&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-cracks&#34;&gt;The Cracks&lt;/h3&gt;
&lt;p&gt;The empirical and theoretical arguments seem fair, but let&amp;rsquo;s see if they stand up to some scrutiny.&lt;/p&gt;
&lt;p&gt;As far back as 95’ there have been works showing that variance remains low for models with increasing complexity.
Curiously, inconsistencies between classical and neural models are demonstrated even in the 92&#39; paper that popularized the tradeoff:&lt;/p&gt;
&lt;img src=&#34;image7.png&#34; width=600 height=600 align=&#34;middle&#34;&gt;
&lt;p&gt;but the authors dismiss them as related to the training procedure and not a fundamental property of the model class.&lt;/p&gt;
&lt;p&gt;As for the generalization bounds, they are too loose to accurately reflect the test error’s trend in practice.&lt;/p&gt;
&lt;p&gt;While these were not sufficient to change the community’s opinion on the tradeoff, it seems that the tide has started to shift in the last 4 years.
In 2019 
&lt;a href=&#34;https://www.pnas.org/doi/10.1073/pnas.1903070116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mikhail Belkin and colleagues&lt;/a&gt; coined &lt;em&gt;double descent&lt;/em&gt; to describe a fairly prevalent phenomenon where the
standard bias-variance picture breaks down once zero training error is attained - what they call the
&lt;em&gt;interpolation threshold&lt;/em&gt;. Before the interpolation threshold, the tradeoff holds and increasing model
complexity leads to overfitting. After the interpolation threshold, however, they found that test error actually starts to go down as you keep increasing model complexity.
This is shown for fully connected networks, random forest and a random fourier feature model.&lt;/p&gt;
&lt;img src=&#34;image9.png&#34;&gt;
&lt;p&gt;Concurrent work by 
&lt;a href=&#34;https://mltheory.org/deep.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nakkiran et al.&lt;/a&gt; has proved that double descent w.r.t &lt;em&gt;model width&lt;/em&gt; is a robust phenomenon that occurs in a variety of tasks, architectures (CNNs, ResNets, Transformers), and optimization methods.&lt;/p&gt;
&lt;img src=&#34;image2.png&#34;&gt;
&lt;p&gt;But more interesting is the phenomenon of training time double descent - training for more epochs decreases performance (before improving again),&lt;/p&gt;
&lt;img src=&#34;image3.png&#34;&gt;
&lt;p&gt;And dataset size double descent - using more training samples (x4.5 more in their experiments) decreases performance before improving again.&lt;/p&gt;
&lt;img src=&#34;image1.png&#34;&gt;
&lt;p&gt;In some experiments, a single descent (rather than double descent) was observed. But a U-shaped loss curve was never observed for neural nets! Which aligns with the modern practice of “bigger is better”.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;reconciling-theory-and-practice&#34;&gt;Reconciling theory and practice&lt;/h3&gt;
&lt;p&gt;There are some suggestive explanations for this deflection between theory and practice.
Many revolve around the notion of complexity - that we just don’t have a proper measure of it.
One for which larger models could actually be simpler.
Regardless of whether you accept one of the hypotheses, you can’t argue with the science.
After all, theory is subordinate to science, as this

&lt;a href=&#34;https://twitter.com/tomgoldsteincs/status/1484609273162309634?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great talk&lt;/a&gt;
by Tom Goldstein calls for.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1912.08286&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brady Neal’s work&lt;/a&gt; on this very subject is subtitled “Textbooks need an update”.
And indeed recent books include double descent in their discussion of the bias variance tradeoff.
Examples include 
&lt;a href=&#34;https://hastie.su.domains/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISLRv2&lt;/a&gt;,

&lt;a href=&#34;https://di.ens.fr/~fbach/ltfp_book.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LTFP&lt;/a&gt;
and 
&lt;a href=&#34;https://mlstory.org/pdf/patterns.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML Story&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Still, more effort should be put in teaching that the bias variance tradeoff is not a universal truth, at least based on the measures of complexity we use in our everyday practice. This has implications on how we select and evaluate ML models.&lt;/p&gt;
&lt;p&gt;As a personal note, what’s still missing for me is a rule of thumb for determining if some case of overfitting
(train error &amp;laquo; test error) is benign, or should it suggest that a different model should be selected.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>On the Hazards of Proxies II</title>
      <link>https://tom-beer.github.io/post/proxies_ml/</link>
      <pubDate>Thu, 28 Apr 2022 10:13:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/proxies_ml/</guid>
      <description>&lt;p&gt;After reviewing the implications of using proxy variables in the context of causal inference,
this post will provide a similar review for the context of prediction models.
This post uses three short stories that demonstrate the problem, with a discussion following.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-riddle&#34;&gt;A Riddle&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with a riddle, presented in 
&lt;a href=&#34;https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MO (2017)&lt;/a&gt;.  It goes like this:&lt;/p&gt;
&lt;p&gt;You are given a risk score model for stroke prior to it being deployed in a hospital&amp;rsquo;s ER for the purpose of triaging patients.
This is an important task as symptoms might be subtle for life-threatening conditions like stroke.&lt;/p&gt;
&lt;p&gt;The training data consists of ~200K visits to the ER of the said hospital. The predictors used are demographic
data and also any prior diagnoses present in the EHR system over the year before
the ER visit. The label is a diagnosis of stroke in the week of the ER visit, and the task is modelled using logistic regression.&lt;/p&gt;
&lt;p&gt;The reported PPV and other metrics seem great, but being a good data scientist you decide to look at the top predictors for future stroke.
Since this is a linear model without interaction terms, the top coefficients might be sufficient for gaining some insight into the model.
You see the following:&lt;/p&gt;
&lt;img src=&#34;1coefs.png&#34; width=600 height=600 align=&#34;middle&#34;&gt;
&lt;p&gt;Prior stroke – that’s expected. CVD history – makes sense. But accidental injury? Colonoscopy? Sinusitis? How can these increase the likelihood of stroke? What’s happening here?
Did the model uncover new risk factors for stroke, unknown to the medical literature?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-answer&#34;&gt;The Answer&lt;/h2&gt;
&lt;p&gt;To figure this out, it’s useful to understand what we are actually predicting here, because it surely isn’t just stroke.
We are not predicting biological stroke, but the presence of a billing code or a doctor&amp;rsquo;s note indicating stroke.
There is a vast difference between the two. For a stroke to be recorded in the data, a patient has to have stroke-like symptoms, decide to seek medical care, get tested and diagnosed with stroke.
Meaning the model predicts a patient’s &lt;strong&gt;propensity to seek care&lt;/strong&gt; just as much as it predicts stroke. In other words, stroke measured in the data is the combination of real biological stroke with &lt;strong&gt;heavy utilization&lt;/strong&gt; of the medical system.
Would we want this model deployed? Of course not.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;story-2-chronic-conditions&#34;&gt;Story 2: chronic conditions&lt;/h2&gt;
&lt;p&gt;The stroke model was trained (from real data) to prove a point, but it was not intended to be deployed at any time.
The second story is about a model that was actually used to generate predictions at a massive scale - for about 70 million Americans each year.
Chronically ill patients receive poor healthcare, so algorithms are used extensively in attempt to target patients into ‘high risk management&#39; programs.
The motivation is to anticipate which patients are at risk for hospitalization, disease flare up, mortality etc., and then intervene to prevent these.
Unfortunately, this algorithm was racially biased (this was subsequently fixed with the help of the researchers that uncovered the bias).&lt;/p&gt;
&lt;p&gt;The decision process goes like this. The model generates a risk score, and then the top 2% of risk scores are enrolled to the high risk program.
The bias can be seen as easily as plotting the risk score percentile against the comorbidity score (stratified by race).&lt;/p&gt;
&lt;img src=&#34;2chronic.png&#34; width=700 height=700 align=&#34;middle&#34;&gt;
&lt;p&gt;We see that or each predicted risk score percentile, Blacks have more active chronic illnesses.
How did this bias come about? The model attempted to predict healthcare needs, and used medical expenditure as the prediction target.
But Blacks and Whites have different relations between health status and cost (due to many reasons).
In short, Black patients cost less conditional on health. Equating expenditure and healthcare needs
generated this disparity. This story was uncovered by 
&lt;a href=&#34;https://www.nature.com/articles/s41591-020-01192-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OPV (2019)&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;story-3-knee-pain&#34;&gt;Story 3: Knee Pain&lt;/h2&gt;
&lt;p&gt;Pain in general is concentrated in society’s most disadvantaged. For example, Blacks report pain twice as frequent compared to other ethnicities.
Knee pain is one condition where these disparities are present.
The routine treatment protocol for knee pain is to have an X-ray evaluated by a radiologist.
Osteoarthritis severity is measured from the X-ray based on standard scores like the Kellgren–Lawrence grade.
Then the patient is commonly referred to either knee replacement surgery or physiotherapy.&lt;/p&gt;
&lt;p&gt;Unfortunately, it has been shown many times that pain disparities remain even when adjusting for
radiographic osteoarthritis severity (KLG) -
Black, lower income and lower education patients have more pain at every level of X-ray graded disease severity.&lt;/p&gt;
&lt;p&gt;This doesn&amp;rsquo;t make sense. Why are radiologists not able to see pain the same for all people?
Because radiographic severity is not a credible predictor for pain. This might have to do with the fact that
KLG was developed in the 50s on White British males, while being applied today on the whole population,
as discussed in 
&lt;a href=&#34;https://www.nature.com/articles/s41591-020-01192-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PCL+ (2021)&lt;/a&gt;.&lt;/p&gt;
&lt;img src=&#34;3pain.png&#34; width=600 height=600 align=&#34;middle&#34;&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tying-it-up&#34;&gt;Tying it up&lt;/h2&gt;
&lt;p&gt;The recurring theme in all three failure cases is unintended implications of using proxy variables in predictive modelling. In the first case, stroke recorded in the ER data was a proxy for stroke in the general population.
In the second case, medical expenditure was a proxy for healthcare needs. And in the third, the radiologist’s diagnostic label was a proxy for the true underlying pain.&lt;/p&gt;
&lt;p&gt;If we think about it, these examples are not exceptions. In most predictive tasks, we are interested in modelling unobservable theoretical constructs, like socioeconomic status, patient health, teacher effectiveness etc.
Because these constructs cannot be observed, they cannot be measured directly. Instead, they must be inferred from measurements of observable properties. Collapsing the distinction between unobservable constructs and observed data might cause adverse implications,
while acknowledging this gap is the best thing we can do to anticipate (and maybe mitigate) these implications.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;further-reading&#34;&gt;Further reading&lt;/h2&gt;
&lt;p&gt;The theory of measurement modelling formalizes a statistical model between the latent construct and its measured proxies.

&lt;a href=&#34;https://arxiv.org/abs/1912.05511&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This work&lt;/a&gt; is a great read on the topic.&lt;/p&gt;
&lt;p&gt;Aside from fairness related harms like discussed here, when using proxies we can expect a mismatch between the predictive task and the real world objective.
This might also lead to misleading evaluation of the model -see this 
&lt;a href=&#34;https://slideslive.com/38938219/evaluating-ml-decision-support-tools-as-embedded-systems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great talk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ever seen claims of superhuman medical performance? It is likely because the model performs a proxy for the real medical task, see for example the discussion 
&lt;a href=&#34;https://t.co/tpsZufYZ7G&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;takeaway&#34;&gt;Takeaway&lt;/h2&gt;
&lt;p&gt;Proxy variables and proxy tasks have broad implications for the utility and fairness of the models in which they are used.
By making explicit the gap between the objective and its proxy, we might be able to build better ML systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Responsible Automation by Learning to Reject</title>
      <link>https://tom-beer.github.io/post/reject/</link>
      <pubDate>Wed, 19 May 2021 10:13:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/reject/</guid>
      <description>&lt;p&gt;This post introduces the framework of rejection learning with applications to algorithmic automation problems.&lt;/p&gt;
&lt;h2 id=&#34;responsible-prediction&#34;&gt;Responsible Prediction&lt;/h2&gt;
&lt;p&gt;In high stakes domains like healthcare, machine learning systems must exhibit some sort of accountability and sensibility for them to be safely deployed. While it is still a difficult and ill-defined task, there are many approaches to making prediction models behave responsibly. For example, we can manipulate the raw model outputs, to make sure that it alligns with a predefined set of rules. Another example is to defer the decision to a simpler model, like a medical protocol.&lt;/p&gt;
&lt;p&gt;Using a rejection mechanism is one aspect often neglected in the design of prediction models. When a model has the ability to 
&lt;a href=&#34;https://www.nature.com/articles/s41746-020-00367-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;abstain&lt;/a&gt; from making a prediction, the resulting system is expected to be a lot more sensible. In healthcare, this would mean models that are safer, better performing, and trusted by  physicians.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;automation-neq-prediction&#34;&gt;Automation $\neq$ Prediction&lt;/h2&gt;
&lt;p&gt;The main motivation for rejection learning is algorithmic automation. Many actionable prediction tasks are actually automation problems – we want to replace human effort with algorithms.&lt;/p&gt;
&lt;p&gt;We tend to equate prediction and automation, but by doing that we miss out on challenges and opportunities that arise from the differences between the two problems:&lt;/p&gt;
&lt;p&gt;Since automation builds upon the stable ground of humans performing a task, in most applications there&amp;rsquo;s no requirement that all instances be automated by the algorithm. Unlike prediction problems that do not build upon humans performing a task, here applying automation to any fraction of the population has a significant potential for impact. Thus algorithmic automation is more than just the replacement of human effort - it also includes the meta decision of which instances to automate. A great overview of this topic is available 
&lt;a href=&#34;https://arxiv.org/abs/1903.12220&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;classification-with-rejection&#34;&gt;Classification with Rejection&lt;/h2&gt;
&lt;p&gt;In rejection learning, we extend the multiclass output space $\mathcal{Y}= \{ 1,\ldots,K\}$ to include $\circledR$ which denotes rejection. In ordinary classification we use the usual zero-one loss $\ell_{01}(f(\boldsymbol{x}), y)=\mathbb{1}_{[f(\boldsymbol{x}) \neq y]}$, and in rejection learning we use an extension of it &amp;ndash; the zero-one-c loss:&lt;/p&gt;
&lt;p&gt;$$
\ell_{01c}(f(\boldsymbol{x}), y) = \begin{cases}
c &amp;amp; f(\boldsymbol{x}) = \circledR \ \newline
\ell_{01}(f(\boldsymbol{x}), y) &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;This loss funcion means that we pay a cost of $c$ if we choose to reject the instance, and otherwise pay a cost of $1$ for a missclassification.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;optimal-classifier&#34;&gt;Optimal Classifier&lt;/h2&gt;
&lt;p&gt;The optimal solution for a function minimizing the expected risk w.r.t $\ell_{01c}$ was formulated backn in the 1950s by 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/5222035&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chow&lt;/a&gt;:
$$
f^*(x) = \begin{cases}
\circledR &amp;amp; \text{max}_y \text{ }P(y|x) \le 1-c \ \newline
\text{argmax}_y \text{ }P(y|x) &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;This classification rule dicatates that we should reject an instance if the posterior probabilities for all $K$ classes are smaller than $1-c$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;determining-the-rejection-cost-c&#34;&gt;Determining the rejection cost $c$&lt;/h2&gt;
&lt;p&gt;The cost of rejection is a hyperparameter we need to set before learning a classification rule. A larger cost means that more instances are rejected, so it should be set by considering a tradeoff between the coverage rate (the fraction of accepted cases) and cost of missclassification.&lt;/p&gt;
&lt;p&gt;A discussion on this tradeoff follows towards the end of the post, but a rule of thumb applicable for any task is this: $c$ should be smaller than the cost induced by a random guess (otherwise we would obtain an unhelpful, trivial classifier). Assuming the classes are balanced, this rule of thumb implies that:&lt;/p&gt;
&lt;p&gt;$$
c \in \bigg(0, \frac{K-1}{K}\bigg),
$$&lt;/p&gt;
&lt;p&gt;and in the general case of imbalanced data, $c$ should be smaller than one minus the smallest class size.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;two-approaches&#34;&gt;Two approaches&lt;/h2&gt;
&lt;p&gt;How can we learn a classification with rejection rule? The literature describes two possible approaches.&lt;/p&gt;
&lt;p&gt;First is the &lt;strong&gt;confidence approach&lt;/strong&gt;. Chow’s optimal rule suggests that classification with rejection is solved if we know the posterior distributions of the classes given the data. Hence the confidence approach is a straightforward solution: train a classifier and use the output of the classifier as a confidence score, drawing inferences by substituting the confidence scores in Chow&amp;rsquo;s rule. The main drawback here is that reliabely estimating class-posterior probabilities is difficult (to say the least). For example, neural networks are notoriously known for their overconfident scores.&lt;/p&gt;
&lt;p&gt;Second is the &lt;strong&gt;classifier-rejector&lt;/strong&gt; approach. If we take another look at Chow&amp;rsquo;s rule, we can rewrite it as two functions:&lt;/p&gt;
&lt;p&gt;$$
r^*(x) = \text{max}_y \text{ }P(y|x) - (1-c)
$$&lt;/p&gt;
&lt;p&gt;$$
h^*(x) = \text{argmax}_y \text{ }P(y|x)
$$&lt;/p&gt;
&lt;p&gt;where $h(x)$ is an ordinary classifier and $r(x)$ is a rejector, rejecting instances if it&amp;rsquo;s value is negative. This formulation suggests that a classification rule could be obtained by learning two independent functions. Apart from a neat logical separation, this approach is more flexible compared to the confidence approach. For example, different hypotheses spaces could be used to model the classifier and rejector. A simple illustration for the increased flexibility of the classifier-rejector approach was brought in 
&lt;a href=&#34;https://cs.nyu.edu/~mohri/pub/rej.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; &amp;ndash; for the case of linear models, only decoupling between the classifier and rejector can solve this data:&lt;/p&gt;
&lt;img src=&#34;flexibility2.png&#34; width=550 height=550 align=&#34;middle&#34;&gt;
&lt;p&gt;Many heuristics fit in this approach – for example, we can use an outlier detection algorithm to reject some instances, and learn a classifier independently.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;theoretical-guarantees&#34;&gt;Theoretical Guarantees&lt;/h2&gt;
&lt;p&gt;Over the years, many solutions of both approaches have been proposed for the problem of learning with rejection. Unfortunately, none of them were able to show theoretical justifications for the muliclass case &amp;ndash; only for binary classification.&lt;/p&gt;
&lt;p&gt;The reason for this is that it is hard to satisfy calibration for the learned classifier, which is required in order to say something about the classifier&amp;rsquo;s behaviour. This point is discussed in detail 
&lt;a href=&#34;https://arxiv.org/abs/1901.10655&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-sensitive-learning&#34;&gt;Cost sensitive learning&lt;/h2&gt;
&lt;p&gt;A notable exception in the multiclass case is 
&lt;a href=&#34;https://arxiv.org/abs/2010.11748&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this work&lt;/a&gt;, which managed to propose a theroetically sound solution for this problem. The authors use the framework of cost sensitive learning to derive a learning rule. Cost sensitive learning allows to assign weights to the different missclassification errors. For the binary case, a cost sensitive loss function would look like this:&lt;/p&gt;
&lt;p&gt;$$
l_{01}^\alpha(f(x), y)=\alpha \cdot \mathbb{1}_{[f(x) \neq y=-1]} + (1-\alpha) \cdot \mathbb{1}_{[f(x)=y=+1]}
$$&lt;/p&gt;
&lt;p&gt;And the optimal classification rule for with respect to this loss is:&lt;/p&gt;
&lt;p&gt;$$
f_\alpha^*(x) = \begin{cases}
+1 &amp;amp; P(y=+1|x) \gt \alpha \ \newline
-1 &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;A key observation in this work is that the problem of learning with rejection could be reduced to an instance of cost sensitive learning using the following rule:&lt;/p&gt;
&lt;p&gt;$$
f^*(x) = \begin{cases}
+1 &amp;amp;  f_{1-c}^*(x)  = +1 \ \newline
-1 &amp;amp;  f_{c}^*(x)  = -1 \ \newline
\circledR &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Where we replace the $\alpha$ parameter of cost senstive learning with $c$ and $1-c$ of the rejection framework.&lt;/p&gt;
&lt;p&gt;Similarily, to extend this rule to the more interesting case of multiclass classification, we can use this reduction:&lt;/p&gt;
&lt;p&gt;$$
f^*(x) = \begin{cases}
\circledR &amp;amp; \max_y  f_{1-c}^{*,y}(x) = -1 \ \newline
\text{argmax}_y  f_{1-c}^{*,y}(x) &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Where the classifiers $f^{*,y}(x)$ denote a one-versus-rest classification for the class $y$.
This rule is equivalent to Chow&amp;rsquo;s rule, thus this reduction implies that we can learn a cost sensitive classifier to solve the rejection learning problem. Further details, including learning and inference rules are brought in this work, but they will not be covered here.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;accuracy-rejection-curves&#34;&gt;Accuracy-rejection curves&lt;/h2&gt;
&lt;p&gt;Just like ordinary classification where we have to strike a tradeoff between the different errors, or optimize a predefined balance between them (e.g. F1 score or AUROC), in rejection learning we need to decide about the balance between some measure of performance (e.g. accuracy) and the coverage rate. The higher the required performance, the lower the achievable coverage rate.&lt;/p&gt;
&lt;p&gt;Accuracy-rejection curves, introduced 
&lt;a href=&#34;http://proceedings.mlr.press/v8/nadeem10a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, are a convenient visualization tool to aid in this decision. These plots are easily understood by stakeholders, and can serve as basis for discussion with them. In addition, accuracy-rejection curves allow to compare different models throughout the coverage axis.&lt;/p&gt;
&lt;p&gt;An interesting point to mention is that practitioners often believe that the better the model at classifying the whole population (without rejection), the better it would be for any non-zero rejection rate. But this is not true, and it is important to compare models on the whole spectrum of rejection. Accuracy-rejection curves can highlight this non-monotonic behaviour.&lt;/p&gt;
&lt;img src=&#34;arc.png&#34; width=550 height=550 align=&#34;middle&#34;&gt;
&lt;hr&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Classification with rejection is a very useful framework when applying prediction models to automation problems. I have a feeling that it is either underutilized, or used but without the proper framing and tooling discussed here. Practitioners may apply some hard-coded filters to reject error-prone cases, but they usually won&amp;rsquo;t use an algorithm to decide which instances to accpet or reject. In the few cases of using a model for rejection, it would probably be some simple hueristic like an outlier detector or using the raw output scores of the classifer.
I believe that a proper adoption of this framework would greatly benefit algorithmic automation problems, and I hope to see it receive more attention in the upcoming years.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the Hazards of Proxies I</title>
      <link>https://tom-beer.github.io/post/proxies_ci/</link>
      <pubDate>Tue, 23 Feb 2021 20:13:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/proxies_ci/</guid>
      <description>&lt;p&gt;This is part one (of at least two) posts about the hazards of using proxy variables in statistical/ML models. Proxy variables are pervasive in almost every data-driven analysis. Part one will focus on the case of observational causal inference, where the main harm is severe bias of the treatment effect estimates.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;confounding&#34;&gt;Confounding&lt;/h2&gt;
&lt;p&gt;Confounding is one of the fundamental challenge to identifying and estimating causal effects. It is a fundamental challenge because the condition of unconfoundedness (sometimes termed ignorability), which is a prerequisite for causal inference, is unverifiable from the data alone. Meaning the observed distribution can be explained by many different data generating processes (DGPs). For a simple illustration, consider the following two DGPs (created by David Lopez Paz):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.randn(N)
y = 0.701 * (x + np.randn(N))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z = 0.8410 * np.randn(N)
x = z + 0.5412 * np.randn(N)
y = z + 0.5412 * np.randn(N)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both cases, the observed distribution $P(X,Y)$ is identical. Yet in the second case, without measuring $z$ and accounting for it, we might be misled to think that $x$ and $y$ have a causal effect on one another. The challenge is that we would never be able to know for sure that all confounders were measured and collected. This challenge leads many practitioners to collect and account for as many variables as they possible can - a &lt;em&gt;&amp;lsquo;throw in the kitchen sink&amp;rsquo;&lt;/em&gt; approach.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;proxies-of-confounders&#34;&gt;Proxies of confounders&lt;/h2&gt;
&lt;p&gt;That&amp;rsquo;s good in theory. But in most real-world observational studies, true ignorability of the treatment assignment doesn’t hold, because we can’t accurately measure all possible confounders. Let&amp;rsquo;s say we want to measure socioeconomic status (SES) because it potentially confounds the effect of red wine intake on the risk for CVD. But what is SES anyway? It’s an abstract concept, never to be measured directly. Same goes for intelligence, or health.&lt;/p&gt;
&lt;p&gt;Luckily we may have access to proxies for these constructs. E.g. ZIP code, years of education or income for SES, IQ test score for intelligence and BMI, blood tests for health status etc. Proxies are everywhere! It can even be argued that all variables are proxies for the latent constructs we would have wanted to measure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;using-proxy-variables&#34;&gt;Using proxy variables&lt;/h2&gt;
&lt;p&gt;How should one use a proxy variable  in a causal analysis? Should we control (adjust) for them, or not? Technically they’re not confounders, because they don’t lie on a backdoor path between treatment and outcome. The following is a graphical representation for a proxy of a confounder.&lt;/p&gt;
&lt;img src=&#34;figure3.2.png&#34; width=400 height=400 align=&#34;middle&#34;&gt;
&lt;p&gt;On the other hand, these proxies are associated with the unmeasured confounders, and so may indirectly adjust for some of the confounding bias. The widespread common practice, whether intentionally acknowledging the covariates as proxies or not, is to use them just like true confounders. For example, a 
&lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent textbook&lt;/a&gt; (highly recommended!) states that it is typically preferable to adjust, rather than not to adjust, for confounder proxies.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;residual-confounding&#34;&gt;Residual confounding&lt;/h2&gt;
&lt;p&gt;What are the implications for this analysis choice? Confounding bias &lt;strong&gt;will not be eliminated&lt;/strong&gt;, no matter how many proxies we gather and irrespective of sample size! To demonstrate this phenomenon of residual confounding, consider the above DAG with $X = U + noise$ for increasing noise intensity. The following is an illustration of the observed data, and the title contains the noise intensity and estimated average treatment effect (ATE).&lt;/p&gt;
&lt;img src=&#34;proxy.gif&#34; width=400 height=400 align=&#34;middle&#34;&gt;
&lt;p&gt;The true causal effect is constant and equals to 1. The confounding is significant: without considering $X$, the average associational difference between treatment groups is about 2.4. When $X$ is accurately measured (setting $\sigma=0$), the causal effect is consistently estimated from data (up to finite sample errors; low sample size was chosen for visibility). But with measurement error, the ATE estimate is biased. The corruption mechanism is the infamous attenuation bias. Unlike simple attenuation bias which always shrinks the regression coefficient towards zero, the bias in ATE estimation can go either way, depending on the data generating process.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;More details for the above simulation&lt;/b&gt;&lt;/summary&gt;
&lt;p&gt;Consider the following DGP: a one-dimensional latent (unobserved) confounder $U$ generates noisy measurements $X$. The treatment $T$ is assigned according to the latent confounder and the outcome $Y$ is a function of the treatment and the latent confounder. The specific dependency structure of this DGP is given below.&lt;/p&gt;
&lt;p&gt;\begin{align*}
U \sim \mathcal{N}(0,1)
\end{align*}
\begin{align*}
X \mid U \sim \mathcal{N}(U,\sigma)
\end{align*}
\begin{align*}
T \mid U \sim \text{Bernoulli}(\text{Sigmoid}(2.5 \cdot U))
\end{align*}
\begin{align*}
Y \mid U,T \sim \mathcal{N}(U+T,1)
\end{align*}&lt;/p&gt;
&lt;/details&gt;
&lt;/br&gt;
&lt;p&gt;In the limit of infinite variance for the measured covariates&#39; noise, the estimated causal effect would converge to the associative difference between treatment groups, as the covariates would lose all information they hold.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;prevalence-and-mitigation&#34;&gt;Prevalence and mitigation&lt;/h2&gt;
&lt;p&gt;To what extent does this issue impact empirical causal inference analyses &amp;ndash; what is its prevalence and magnitude? It is hard to know. A few studies tried to shed light on this by measuring potential confounders &lt;em&gt;and&lt;/em&gt; proxies for these variables, like in 
&lt;a href=&#34;https://link.springer.com/article/10.2307/2648118&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this work&lt;/a&gt;. Other studies managed to show that attenuation bias did indeed occur, like in 
&lt;a href=&#34;https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12146?casa_token=d0jDi0c8ROAAAAAA%3AyM6kJWHzFZSX8guP7ayeWKnWwqDz5Mv5SAiTw0bZrKfcnMIWD-R0XE-piFFzlXouuzCA7r0watzzrg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this work&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some works have attempted to find solutions for this obstacle, dating back to the 
&lt;a href=&#34;https://www.jstor.org/stable/1422689&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;beginning of the previous century&lt;/a&gt;, but this has proved to be a challenging task. Suggested methods either require stringent assumptions or offer no guarantees on the quality of the solution. Notable examples are works using 
&lt;a href=&#34;https://papers.nips.cc/paper/2018/file/86a1793f65aeef4aeef4b479fc9b2bca-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matrix factorization&lt;/a&gt; and 
&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;summary-and-implications&#34;&gt;Summary and implications&lt;/h2&gt;
&lt;p&gt;Residual confounding bias due to the use of proxy variables is a significant obstacle in non-experimental causal inference. Its implications are potentially severe, as the bias could vary in sign and magnitude, making causal effect estimates unreliable. Due to both the limitations of the proposed solutions and an inadequate awareness of the harms described here, the present state of empirical practice is unsatisfactory - none of the proposed methods have gained adoption by practitioners, who continue to produce error-prone analyses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Deep Networks for Scientific Discovery in Physiological Signals</title>
      <link>https://tom-beer.github.io/publication/discovery/</link>
      <pubDate>Wed, 26 Aug 2020 21:54:37 +0300</pubDate>
      <guid>https://tom-beer.github.io/publication/discovery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Networks for Scientific Discovery</title>
      <link>https://tom-beer.github.io/post/deep-scientific-discovery/</link>
      <pubDate>Tue, 21 Jul 2020 20:13:42 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/deep-scientific-discovery/</guid>
      <description>&lt;p&gt;This post will summarize the main points in our recent work - 
&lt;a href=&#34;https://arxiv.org/abs/2008.10936&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Deep Networks for Scientific Discovery in Physiological Signals&lt;/a&gt;, which &lt;del&gt;will be&lt;/del&gt; was presented at the 2020 Machine Learning for Healthcare Conference.
Code for the framework and experiments can be found 
&lt;a href=&#34;https://github.com/tom-beer/deep-scientific-discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Despite the tremendous breakthroughs that deep learning models have brought in the last decade, they remain largely undeployed in high stakes domains like healthcare. Clinicians do not trust the outputs of complex black box models, and prefer to work with simple, interpretable systems that base their decisions on theoretically grounded and understood features.&lt;/p&gt;
&lt;p&gt;For example, in the case of cardiac abnormality detection from ECG signals, practitioners are likely to trust a simple decision tree that uses features they understand like variance of the heartbeat interval series, energy content in different parts of the signal etc.; They likely won&amp;rsquo;t trust a classifier that bases its decisions on deep representations. See this very good survey on 
&lt;a href=&#34;https://arxiv.org/pdf/1905.05134.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;what clinicians want&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But it&amp;rsquo;s exactly the super efficient representation extraction capabilities that make deep learning models so special. End to end models trained in hours exceed or compare to performance of systems that are based on decades-long research and engineering of useful features. In uncharted areas (for example if a new sensing modality would be invented), deep learning models would prove useful from day zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if we could extract interpretable, clinically sound features from the neural network&amp;rsquo;s latent representation, to be used in simple machine learning models? Then these models would be easily deployed in healthcare and other high stakes domains. This is the main motivation for our work.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;main-idea&#34;&gt;Main idea&lt;/h2&gt;
&lt;p&gt;To do that, we need two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A way to understand which aspect of the signal the latent representation captures and focuses on. We use 
&lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;class activations maps&lt;/a&gt; (CAMs) for this aim.&lt;/li&gt;
&lt;li&gt;A way to make the network &lt;em&gt;remove&lt;/em&gt; a feature from the latent representation&amp;rsquo;s hypothesis space. We solve this by modifying the network&amp;rsquo;s architechture and loss function: concatenating the external feature to the latent representation layer, and imposing statistical independence between the two.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The reason we need a mechanism to &lt;em&gt;unlearn&lt;/em&gt; a predefined feature from the network&amp;rsquo;s representation is that we imagine an iterative discovery process. At first, no domain knowledge is available, and so we can learn the task without constraints, using some visualization scheme (like CAMs) to derive a first set of useful, interpretable features. Next, we would like to extract only the residual information - what is not encoded in the first set of features. To do this we need a notion of independence, or orthogonality, between two representations. Only when the network&amp;rsquo;s representation is independent from the already known features could we hope to discover a new feature set. This process goes on and on, until no more information is extracted from the network.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;Achieving this goal can be broken down into 3 steps: Building the constrained representation, ensuring its validity (that it&amp;rsquo;s indeed independent, for example) and visualizing the activations to extract features.&lt;/p&gt;
&lt;h3 id=&#34;construction-of-the-constrained-representation&#34;&gt;Construction of the constrained representation&lt;/h3&gt;
&lt;p&gt;We assume we have access to a set of $n$ samples of the form $(x_1,f_1,y_1), &amp;hellip; , (x_n,f_n,y_n)$, where: $x_i$ are raw signals, for example ECG signals; the vectors $f_i$ are hand-engineered features, calculated as a deterministic function of $x_i$, and typically of much lower dimension than $x_i$; and, $y_i \in [1,\ldots,k]$ are discrete labels.&lt;/p&gt;
&lt;p&gt;The main goal of the architecture and loss function proposed below are to learn DNN features (denoted $g_i$) which are on the one hand informative about the labels $y_i$, but on the other hand contain as little information as possible about the hand engineered features $f_i$. This is achieved by combining two elements: a term added to the objective function which encourages statistical independence between the $f_i$ and $g_i$, and an architecture which encourages $g_i$ to be non-redundant with respect to $f_i$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Independence measure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The measure chosen to enforce independence between the hand-engineered features $f_i$ and the DNN&amp;rsquo;s representation $g_i$ is the Hilbert-Schmidt Independence Criterion (HSIC), which is based on the eigenspectrum of covariance operators  in  reproducing  kernel  Hilbert  spaces.&lt;/p&gt;
&lt;p&gt;There are various other measures of statistical independence; the advantage of HSIC is that it is non-parametric, unlike mutual information, and so it is fit for samples not following a prescribed distributional form. In addition, it does not require training an additional model (i.e. an inference network for variational approximation or an adversarial network).&lt;/p&gt;
&lt;p&gt;HSIC can be thought of as a non-linear extension of the cross-covariance between two random variables. Unlike the cross-covariance, the HSIC between two random variables $X$ and $Y$ equals $0$ if &lt;em&gt;and only if&lt;/em&gt; $X$ is independent of $Y$ (under certain regularity conditions).&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;More on HSIC&lt;/b&gt;&lt;/summary&gt;
        This HSIC loss is calculated in the following manner. For $\mathcal{F, G}$ RKHSs with universal kernels $k$, $l$ we compute the kernel matrices $K_{ij} = k(f_i,f_j)$ and $L_{ij} = l(g_i,g_j)$. Both $k$ and $l$ were selected to be Gaussian kernels:
        $$ K_{i,j} = e^{\frac{{|| f_i-f_j ||}^2_2}{\sigma^2}} $$
        With $L_{i,j}$ computed in a similar manner. Then the scaled Hilbert-Schmidt norm of their cross covariance matrix is calculated:
        $$
        \widehat{HSIC}(\{(f_i,g_j\}^n_{i=1}; \mathcal{F, G} ) = \frac{1}{(n-1)^2}  \cdot \textbf{tr}(KHLH)
        $$
        where $H_{ij}=\delta_{i,j}-\frac{1}{n}$ is a centering matrix.
&lt;/details&gt;
&lt;/br&gt;
&lt;p&gt;&lt;strong&gt;Setting the bandwidth $\sigma$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For each kernel matrix, the bandwidth $\sigma$ is set to be the median pairwise distance of the data points, a heuristic described 
&lt;a href=&#34;http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/ICML2009-Mooij_[0].pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Since $f$ is a fixed representation space, its median pairwise distance can be computed once before training. However $g$ changes every training step, so its bandwidth is updated accordingly at every training step based on a moving average between the current median pairwise distance and the previously set bandwidth.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Concatentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The hand-engineered features $f$ are usually highly predictive for the selected task. In order to further encourage the DNN latent representation $g$ to be distinct from $f$, the two representations are concatenated before passing them on to the final layer of the network. This is done to prevent, as much as possible, from the network representation $g$ to try and replicate $f$, even when under the HSIC constraint. We have found that it is not possible to achieve both reasonable classification accuracy on the original task and sufficient independence from the external representation without this concatenation step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loss function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The loss function of the network&amp;rsquo;s output is given by:
$$
\lambda \text{HSIC}\left([f_1,\ldots f_n];[g_1,\ldots,g_n]\right) + \sum_{i=1}^n \text{CrossEntropy}\left(\hat{y}_i,y_i\right),
$$
where $\lambda$ is a hyperparameter controlling the degree of independence induced between the DNN representation $g_i$ and the hand-engineered representations $f_i$. This parameter is tuned to ensure both high performance on the original task and sufficient independence relative to the external features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Schematic view&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If that sounded complicated, maybe a diagram would clear things up:
&lt;img src=&#34;flow.png&#34; width=550 height=550 align=&#34;middle&#34;&gt;&lt;/p&gt;
&lt;p&gt;This concludes what we refer to as the &lt;em&gt;main task&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;validation-of-the-representation&#34;&gt;Validation of the representation&lt;/h3&gt;
&lt;p&gt;So, we trained a neural network and validated that it yields a reasonable classification accuracy, but what guarantees do we have that (1) the network has succeeded in achieving true independence between the hand-engineered features and internal network representation while (2) retaining some information about the label within the DNN representation? We define two auxiliary tasks whose aim is to boost our confidence in the claims made in the main task&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Independence&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The HSIC term in our loss function is meant to induce statistical independence between the hand-engineered features $f$ and the DNN features $g$. We validate that this is indeed achieved by checking whether one can predict the features $f$ when given the features $g$ as input. We do this by training a multi-task DNN whose input is $g$ and output labels are the entries of $f$. We then measure the held-out squared correlation $R^2$, averaged across the dimensions of $f$. An average $R^2$ value close to 0 indicates a high degree of independence. As a further reference point, this value is compared to the $R^2$ value obtained using an network with the same architecture as ours but without the HSIC term (equivalent to setting $\lambda$ = 0), which we call the &lt;em&gt;baseline model&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Residual Information&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To confirm that the obtained representation $g$ holds useful information for the task in question, we run the following evaluation: Our method is trained on a sample set $train_1$ . Then we apply the representation function $g$ to a separate sample set $train_2$. We fit a vanilla 3-layer network, which we call &lt;em&gt;Rep2Label&lt;/em&gt;, predicting $y$ from $g$ on the training set $train_2$. Finally, we evaluate the accuracy of &lt;em&gt;Rep2Label&lt;/em&gt; on a held-out test set. An accuracy greater then chance indicates that the latent representation still holds valuable information about the label and can be analyzed for our goal of scientific discovery.&lt;/p&gt;
&lt;h3 id=&#34;feature-discovery-through-visualization&#34;&gt;Feature discovery through visualization&lt;/h3&gt;
&lt;p&gt;The third and final step in the proposed framework is to visualize the network&amp;rsquo;s activations over different aspects of the input signals using CAMs. CAMs are a technique for visualizing spatially localized class-relevant information. They are obtained by a simple matrix multiplication between a CNN’s activations (before global average pooling) and the weights of the fully connected layer.&lt;/p&gt;
&lt;p&gt;For a physiological signal the output of CAM is, for each sample, a time series showing what parts of the signal produce strong activations per a given class. We (1) align these activation profiles around a meaningful physiological landmark, e.g. an R-peak in ECG signals or a rapid eye movement in EEG signals, (2) normalize the  activation profiles to [0, 1], and (3) average them over the entire test set. When the mere existence of an event is significant, we would expect CAM peak over the event start. This high activation over the event’s start is interpreted as importance for the task.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;some-results&#34;&gt;Some results&lt;/h2&gt;
&lt;p&gt;This approach is demonstrated for two types of signals and tasks: Atrial fibrillation detection in ECG signals and REM detection in EEG signals. Both these tasks were very well studied in the medical literature over the last decades, so we do not aim to discover anything new here. Instead, we aim to discover the already known domain knowledge. Because &lt;em&gt;discovery&lt;/em&gt; is a qualitative notion and we do not have any quantitative measures of success, it was important to select example tasks that could serve as benchmarks for our approach.&lt;/p&gt;
&lt;p&gt;The full results are in the paper, including details about the experimental setup. Here I will focus on one type of result from the ECG use case.
&lt;strong&gt;Data and task&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The dataset used for this experiment is from the 
&lt;a href=&#34;https://physionet.org/content/challenge-2017/1.0.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017 PhysioNet Challenge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The original challenge&amp;rsquo;s task was to classify single-lead ECGs to the following categories: Normal, atrial fibrillation (AF), other and noisy. We modified the task to be a binary classification: detection of AF.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;External features&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When clinicians diagnose AF from ECGs, they focus on 2 types of signal properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Properties of the RR interval sequence. In cases of AF, the heartbeats are irregularly spaced. Hand-engineered features that capture this behavior include median, STD, RMS, multiscale entropy, minimum and maximum of the RR sequence.&lt;/li&gt;
&lt;li&gt;Properties of the P-waves. P-waves are the first positive deflection in a normal heartbeat. In AF, the P-wave may not exist at each beat. Hand-engineered features that characterize this part of the signal include the maximum, STD and energy of the amplitude of the time windows where the P-Wave should appear.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model was trained 4 times: With no features (baseline), with the RR features, with the P-wave features and with all features together. After ensuring that the models have passed the validation tests, the following mean activations were extracted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RR constrained model:&lt;/strong&gt;
&lt;img src=&#34;RR.png&#34; width=550 height=550 align=&#34;middle&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P-wave constrained model:&lt;/strong&gt;
&lt;img src=&#34;PWave.png&#34; width=550 height=550 align=&#34;middle&#34;&gt;&lt;/p&gt;
&lt;p&gt;For the RR model (whose representation is encouraged not to use R-peak related information), we observe an almost smooth  activation with a clear peak which starts around 300ms before the R-peak and ends around 100ms before the R-peak. That is, The RR model’s activation peak has an almost perfect overlap with the expected location of the P-wave window. In addition, in the P-Wave model’s activation (i.e a model that is pushed to not use P-wave-based features), it seems that the activation shifted toward the R-peak starting point. As one of the hallmark features of AF is the loss of regularity of the intervals between consecutive R-peaks, an increase in activation at the expected location of the R-peak seems to reflect the shift of the internal  representation.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;This work shows a proof of concept for the discovery of interpretable features by imposing constraints on neural representations.
There are some limitations, including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The proposed framework is relevant mostly to local, morphological features due to the nature of the class activation maps.&lt;/li&gt;
&lt;li&gt;The real challenge of feature discovery is not nearly solved by this approach. A domain expert needs to translate the activation templates and come up with interpretable functions or scientific understanding that capture the exposed pattern.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>The Synthetic Control Method</title>
      <link>https://tom-beer.github.io/post/synthetic-control/</link>
      <pubDate>Wed, 08 Jul 2020 23:14:30 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/synthetic-control/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;This post was generated from a Jupyter notebook; it can be found &lt;a href=&#39;https://github.com/tom-beer/Synthetic-Control-Examples&#39;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Suppose we want to estimate the causal effect of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An anti smoking taxation program in California on cigarette consumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The German reunification on West Germany’s economy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Right to carry laws on violent crimes in Texas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uber and Lyft on traffic congestion in Paris&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What would be a sensible approach to answer these questions? For practitioners coming from domains like healthcare, advertisement or collaborative filtering, these questions do not seem to fit in their everyday way of thinking. The reason is that for these questions, we are interested in the effect of an intervention for a single unit, while we do not have any data about this intervention for other units.&lt;/p&gt;
&lt;p&gt;Take the German reunification as an example - it is only Germany that was reunified, and the task is to infer the causal effect of that historic event. As for the other examples - sure, California was not the first to implement an anti-smoking program, and Uber is active in many cities over the globe apart from Paris. But the task considered here is to use only the intervention data for the unit in question.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-qualitative-approach&#34;&gt;The qualitative approach&lt;/h3&gt;
&lt;p&gt;It seems insensible to make inferences on a single unit - what credibility could we attain? What would be the statistical properties of the estimator?&lt;/p&gt;
&lt;p&gt;But for decades, qualitative analyses that seem unsensible were carried out by economists and social scientists. They would meticulously find relevant controls for the relevant treated unit, relying on informal statements of affinity between the units. Then, with a chosen control unit, identification schemes like difference-in-differences would be applied.&lt;/p&gt;
&lt;p&gt;This approach has some merit - the resulting estimator is interpretable, transparent and simple. It is easy to communicate it and argue about the choice of control units. Yet it&amp;rsquo;s clear that it is not the optimal approach, at least for practitioners of the quantitative / statistical learning schools. So the question is - can we do better? Can we devise a data-driven approach for causal effect estimation of aggregate interventions? Can we phrase a clear objective for the estimator?&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;synthetic-controls&#34;&gt;Synthetic Controls&lt;/h3&gt;
&lt;p&gt;Introduced in 2003 and formalized in 2010 by Alberto Abadie and colleagues, the synthetic control aims to bridge between the traditional qualitative approaches of the social sciences and the modern data driven approaches. It strikes a balance between the two by offering a clear objective function to optimize while constraining the model to remain transparent, interpretable and simple. It is applicable to almost any domain, and became very popular thanks to its lenient assumptions. Esteemed economists Susan Athey and Guido Imbens have called it &amp;ldquo;arguably the most important innovation in the policy evaluation literature in the last 15 years&amp;rdquo;. Sounds interesting? Let&amp;rsquo;s dive in.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-main-idea&#34;&gt;The main idea&lt;/h3&gt;
&lt;p&gt;The qualitative approach calls for a manual selection of control units from a pool of potential control units. The main idea of the synthetic control method is to forgo any kind of manual control selection. Instead, the method makes use of &lt;strong&gt;all&lt;/strong&gt; the control units in a data-driven approach. The controls are weighted based on their similarity to the treated unit, yielding the synthetic control as a weighed combination of the original units.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Potential outcomes - click here for details&lt;/b&gt;&lt;/summary&gt;
  We can formalize the main idea above in the language of potential outcomes:
&lt;ul&gt;
&lt;li&gt;For the control units, we observe the potential outcome under no-intervention&lt;/li&gt;
&lt;li&gt;For the treated unit, we observe its potential outcome under no-intervention in the pre-intervention period, and its potential outcome under the intervention in the post-intervention period&lt;/li&gt;
&lt;li&gt;To calculate the causal effect, we need the treated unit&amp;rsquo;s potential outcome under no-intervention in the post-intervention period&lt;/li&gt;
&lt;li&gt;The synthetic control emulates this potential outcome. It attempts to answer the question of &amp;ldquo;What would have been the outcome for the treated unit, without the intervention?&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;p&gt;We see that the task of causal effect estimation boils down to finding the optimal weighting of the control units.
And how shall one find the optimal weights? This is the point where the road forks - there are many suggestions for phrasing of the optimization problem, each with its own pros and cons. The next section will describe one possibility, which is the originally proposed apparoch by Abadie et al; A discussion of its limitations in face of more contemporary approaches will follow. But first - some (minimal) notation.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-setting&#34;&gt;The setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We have data for $T$ time periods, $T_0$ of them are pre-intervention and $T_1$ belong to the post-intervention period&lt;/li&gt;
&lt;li&gt;We have $J$ control units, $j\in{1,..,n}$, and one treated unit ($j=0$)&lt;/li&gt;
&lt;li&gt;The measured outcomes for each unit and time period is $Y_{jt} \in \mathbb{R}^{T \times J+1}$&lt;/li&gt;
&lt;li&gt;The synthetic control is defined as $\sum_{j=1}^{J} Y_{jt} \cdot W_j$&lt;/li&gt;
&lt;li&gt;The causal effect estimate is thus $\tau_t = Y_{0t} - \sum_{j=1}^{J} Y_{jt} \cdot W_j$&lt;/li&gt;
&lt;li&gt;In addition, for some methods we require a set of $K$ outcome predictors for each unit: $X_{kj}\in \mathbb{R}^{K \times J+1}$. These may be lagged outcomes (pre-intervention) or other auxiliary features (or both)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;adhs-synthetic-control&#34;&gt;ADH&amp;rsquo;s synthetic control&lt;/h3&gt;
&lt;p&gt;Proposed by Abadie, Diamond and Hainmueller (hence the name) in 
&lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;, this is the seminal proposal for policy evaluation using synthetic controls. Apart from introducing the main idea above, they present the following objective function:&lt;/p&gt;
&lt;p&gt;$$W^* \in argmin_{W \in {\Delta}^{J}} \lVert \mathbf{X_0-X_{\neg0} \cdot W} \rVert  $$&lt;/p&gt;
&lt;p&gt;Some remarks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The similarity between the controls and the treated units is measured with respect to the features $X$, while the synthetic control is constructed using the outcomes $Y$. It might seem more sensible to perform the matching with respect the outcome from the start. After all, we are interested in a good pre-treatment fit for the outcome, and the best fit would be obtained using the outcome and not a proxy for it. Indeed, recent works claim that &amp;ldquo;&lt;em&gt;covariates are of less relevance in applications; outcomes tend to be substantially more important than covariates in terms of predictive power&lt;/em&gt;&amp;rdquo; (
&lt;a href=&#34;https://arxiv.org/abs/1607.00699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see here&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The weights are constrained to lie in the J-simplex, i.e. they should be non-negative and sum to one. The authors impose this constraint to avoid extrapolation and increase the method&amp;rsquo;s transparency. But like the previous point, here too recent studies recommend to remove this constraint, stating that it is not obvious that restricting the weights this way is optimal (
&lt;a href=&#34;https://arxiv.org/abs/1607.00699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see here&lt;/a&gt;). From a different angle, this constraint could be seen as weight regularization, for example 
&lt;a href=&#34;https://arxiv.org/pdf/1803.00096.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this work&lt;/a&gt; shows an equivalence between the simplex constraint and LASSO regularization with a fixed coefficient. The question that arises is - why use a fixed-strength regularization, instead of using LASSO with cross-validation?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are valid and thoughtful concerns. I think it would be best to take the main idea of the synthetic control method, but to experiment with the implementation details for each use case.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;an-example-use-case&#34;&gt;An example use case&lt;/h2&gt;
&lt;h3 id=&#34;estimating-the-effect-of-californias-tobacco-control-program&#34;&gt;Estimating the effect of California’s tobacco control program&lt;/h3&gt;
&lt;p&gt;This section will demonstrate the described method on a specific use case. The case of California&amp;rsquo;s tobacco control program was one of the first applications of the synthetic control method, and it was introduced in 
&lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;. I will recover all steps: data processing, feature extraction, synthetic control construction and causal effect estimation - exactly as shown in the paper and described above (no modifications at all to the original method). If something wasn&amp;rsquo;t clear above - hopefully this example will clarify things up.&lt;/p&gt;
&lt;h5 id=&#34;the-research-question&#34;&gt;The research question&lt;/h5&gt;
&lt;p&gt;In 1988 California implemented a large scale tobacco control program, called proposition 99, that increased  California’s cigarette excise tax by 25 cents per pack, earmarked the tax revenues to health and anti-smoking education budgets, funded anti-smoking media campaigns, and spurred local clean indoor-air ordinances throughout the state. The question we are interested here it to evaluate the policy&amp;rsquo;s effect in terms of per-capita cigarette consumption. The potential controls in this case are all US states (that&amp;rsquo;s an 
&lt;a href=&#34;https://en.wikipedia.org/wiki/RAS_syndrome&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAS&lt;/a&gt;).&lt;/p&gt;
&lt;h5 id=&#34;data&#34;&gt;Data&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;The main outcome is annual per-capita cigarette consumption. The data for this can be found 
&lt;a href=&#34;https://github.com/johnson-shuffle/mixtape/tree/master/data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In addition to the main outcome, we will utilize some features that correlate with it (GDP, ages, beer consumption etc). This data is taken from 
&lt;a href=&#34;https://github.com/jehangiramjad/tslib/tree/master/tests/testdata&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_outcome_raw = pd.read_csv(&#39;prop99.csv&#39;)
df_outcome_raw = df_outcome_raw[df_outcome_raw[&#39;SubMeasureDesc&#39;] == &#39;Cigarette Consumption (Pack Sales Per Capita)&#39;]
df_outcome = pd.DataFrame(df_outcome_raw.pivot_table(values=&#39;Data_Value&#39;, index=&#39;LocationDesc&#39;, columns=[&#39;Year&#39;]).to_records())

rda_predictors = pyreadr.read_r(&#39;smoking.rda&#39;)
df_predictors = pd.DataFrame(list(rda_predictors.values())[0])
print(f&#39;The original dataset contains {df_outcome.LocationDesc.unique().shape[0]} states&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The original dataset contains 51 states
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As part of the method&amp;rsquo;s assumptions and requirements, it is important to exclude from the donor pool (this is how the collection of control units is traditionally called) any unit that may not be a true control - i.e. any unit that has implemented a similar intervention. In our case, some states also introduced anti-smoking programs or substantially increased the tax for cigarettes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bad_states = [&#39;Massachusetts&#39;, &#39;Arizona&#39;, &#39;Oregon&#39;, &#39;Florida&#39;, &#39;Alaska&#39;, &#39;Hawaii&#39;, &#39;Maryland&#39;, 
              &#39;Michigan&#39;, &#39;New Jersey&#39;, &#39;New York&#39;, &#39;Washington&#39;, &#39;District of Columbia&#39;]

df_outcome.drop(df_outcome[df_outcome[&#39;LocationDesc&#39;].isin(bad_states)].index, inplace=True)
ca_id = df_outcome[df_outcome[&#39;LocationDesc&#39;] == &#39;California&#39;].index.item()
df_outcome = df_outcome.reset_index()
df_outcome = df_outcome.rename(columns={&#39;index&#39;: &#39;org_index&#39;})
print(f&#39;After filtering out some states, we are left with {df_outcome.LocationDesc.unique().shape[0]} states (including California):&#39;)
df_outcome.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;After filtering out some states, we are left with 39 states (including California):
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;org_index&lt;/th&gt;
      &lt;th&gt;LocationDesc&lt;/th&gt;
      &lt;th&gt;1970&lt;/th&gt;
      &lt;th&gt;1971&lt;/th&gt;
      &lt;th&gt;1972&lt;/th&gt;
      &lt;th&gt;1973&lt;/th&gt;
      &lt;th&gt;1974&lt;/th&gt;
      &lt;th&gt;1975&lt;/th&gt;
      &lt;th&gt;1976&lt;/th&gt;
      &lt;th&gt;1977&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;2005&lt;/th&gt;
      &lt;th&gt;2006&lt;/th&gt;
      &lt;th&gt;2007&lt;/th&gt;
      &lt;th&gt;2008&lt;/th&gt;
      &lt;th&gt;2009&lt;/th&gt;
      &lt;th&gt;2010&lt;/th&gt;
      &lt;th&gt;2011&lt;/th&gt;
      &lt;th&gt;2012&lt;/th&gt;
      &lt;th&gt;2013&lt;/th&gt;
      &lt;th&gt;2014&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;89.8&lt;/td&gt;
      &lt;td&gt;95.4&lt;/td&gt;
      &lt;td&gt;101.1&lt;/td&gt;
      &lt;td&gt;102.9&lt;/td&gt;
      &lt;td&gt;108.2&lt;/td&gt;
      &lt;td&gt;111.7&lt;/td&gt;
      &lt;td&gt;116.2&lt;/td&gt;
      &lt;td&gt;117.1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;82.4&lt;/td&gt;
      &lt;td&gt;83.3&lt;/td&gt;
      &lt;td&gt;80.2&lt;/td&gt;
      &lt;td&gt;78.0&lt;/td&gt;
      &lt;td&gt;75.6&lt;/td&gt;
      &lt;td&gt;71.5&lt;/td&gt;
      &lt;td&gt;68.4&lt;/td&gt;
      &lt;td&gt;67.2&lt;/td&gt;
      &lt;td&gt;64.6&lt;/td&gt;
      &lt;td&gt;61.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;100.3&lt;/td&gt;
      &lt;td&gt;104.1&lt;/td&gt;
      &lt;td&gt;103.9&lt;/td&gt;
      &lt;td&gt;108.0&lt;/td&gt;
      &lt;td&gt;109.7&lt;/td&gt;
      &lt;td&gt;114.8&lt;/td&gt;
      &lt;td&gt;119.1&lt;/td&gt;
      &lt;td&gt;122.6&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;82.1&lt;/td&gt;
      &lt;td&gt;81.4&lt;/td&gt;
      &lt;td&gt;78.4&lt;/td&gt;
      &lt;td&gt;77.0&lt;/td&gt;
      &lt;td&gt;72.6&lt;/td&gt;
      &lt;td&gt;63.2&lt;/td&gt;
      &lt;td&gt;61.1&lt;/td&gt;
      &lt;td&gt;60.5&lt;/td&gt;
      &lt;td&gt;57.5&lt;/td&gt;
      &lt;td&gt;54.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;123.0&lt;/td&gt;
      &lt;td&gt;121.0&lt;/td&gt;
      &lt;td&gt;123.5&lt;/td&gt;
      &lt;td&gt;124.4&lt;/td&gt;
      &lt;td&gt;126.7&lt;/td&gt;
      &lt;td&gt;127.1&lt;/td&gt;
      &lt;td&gt;128.0&lt;/td&gt;
      &lt;td&gt;126.4&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;33.1&lt;/td&gt;
      &lt;td&gt;32.9&lt;/td&gt;
      &lt;td&gt;31.8&lt;/td&gt;
      &lt;td&gt;30.3&lt;/td&gt;
      &lt;td&gt;28.8&lt;/td&gt;
      &lt;td&gt;26.3&lt;/td&gt;
      &lt;td&gt;26.0&lt;/td&gt;
      &lt;td&gt;25.2&lt;/td&gt;
      &lt;td&gt;23.9&lt;/td&gt;
      &lt;td&gt;22.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;124.8&lt;/td&gt;
      &lt;td&gt;125.5&lt;/td&gt;
      &lt;td&gt;134.3&lt;/td&gt;
      &lt;td&gt;137.9&lt;/td&gt;
      &lt;td&gt;132.8&lt;/td&gt;
      &lt;td&gt;131.0&lt;/td&gt;
      &lt;td&gt;134.2&lt;/td&gt;
      &lt;td&gt;132.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;57.9&lt;/td&gt;
      &lt;td&gt;53.1&lt;/td&gt;
      &lt;td&gt;51.1&lt;/td&gt;
      &lt;td&gt;48.4&lt;/td&gt;
      &lt;td&gt;46.0&lt;/td&gt;
      &lt;td&gt;41.4&lt;/td&gt;
      &lt;td&gt;40.6&lt;/td&gt;
      &lt;td&gt;40.6&lt;/td&gt;
      &lt;td&gt;38.3&lt;/td&gt;
      &lt;td&gt;36.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;120.0&lt;/td&gt;
      &lt;td&gt;117.6&lt;/td&gt;
      &lt;td&gt;110.8&lt;/td&gt;
      &lt;td&gt;109.3&lt;/td&gt;
      &lt;td&gt;112.4&lt;/td&gt;
      &lt;td&gt;110.2&lt;/td&gt;
      &lt;td&gt;113.4&lt;/td&gt;
      &lt;td&gt;117.3&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;49.9&lt;/td&gt;
      &lt;td&gt;50.9&lt;/td&gt;
      &lt;td&gt;50.5&lt;/td&gt;
      &lt;td&gt;47.4&lt;/td&gt;
      &lt;td&gt;45.9&lt;/td&gt;
      &lt;td&gt;40.8&lt;/td&gt;
      &lt;td&gt;36.3&lt;/td&gt;
      &lt;td&gt;33.5&lt;/td&gt;
      &lt;td&gt;31.4&lt;/td&gt;
      &lt;td&gt;30.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 47 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s construct the matrices defined above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_outcome_ca = df_outcome.loc[df_outcome[&#39;LocationDesc&#39;] == &#39;California&#39;, :]
df_outcome_control = df_outcome.loc[df_outcome[&#39;LocationDesc&#39;] != &#39;California&#39;, :]

ca_outcomes_pre = df_outcome_ca.loc[:,[str(i) for i in list(range(START_TIME, INTERVENTION_TIME))]].values.reshape(-1,1)
control_outcomes_pre = df_outcome_control.loc[:,[str(i) for i in list(range(START_TIME, INTERVENTION_TIME))]].values.transpose()

ca_outcomes_post = df_outcome_ca.loc[:,[str(i) for i in list(range(INTERVENTION_TIME, STOP_TIME))]].values.reshape(-1,1)
control_outcomes_post = df_outcome_control.loc[:,[str(i) for i in list(range(INTERVENTION_TIME, STOP_TIME))]].values.transpose()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot the outcomes for California vs. an average of all US states, we see that the &amp;lsquo;average control state&amp;rsquo; is not like California in the pre-intervention period, violating the parallel trends assumption required to draw inferences. Therefore we will find an optimal weighting that would attain a good fit to California in the pre-intervention period.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_outcomes = np.vstack([control_outcomes_pre, control_outcomes_post]).mean(axis=1)
CA_outcomes = np.vstack([ca_outcomes_pre, ca_outcomes_post]).flatten()
fig = plt.figure(figsize=(7.5,4.5))
plt.plot(range(START_TIME,STOP_TIME),mean_outcomes, &#39;r--&#39;, label=&amp;quot;rest of the U.S.&amp;quot;);
plt.plot(range(START_TIME,STOP_TIME),CA_outcomes, &#39;b-&#39;, label=&amp;quot;California&amp;quot;);

plt.ylabel(&#39;per-capita cigarette sales (in packs)&#39;)
plt.xlabel(&#39;year&#39;)
plt.legend(loc=&#39;upper right&#39;)
plt.title(&amp;quot;Figure 1: Trends in per-capita cigarette sales: California vs. the rest of the United States&amp;quot;)
plt.axvline(INTERVENTION_TIME)
plt.text(x=INTERVENTION_TIME+0.2, y=30, s=&#39;Passage of Proposition 99&#39;)
plt.xlim([START_TIME, STOP_TIME-1])
plt.ylim([0, 150])
plt.grid()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/intro_files/output_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s extract the covariates used in the paper.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that some of the boilerplate code has been rendered out of the html file, but it can be found in the original notebook 
&lt;a href=&#34;&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;control_predictors = []
for state in df_outcome[&#39;LocationDesc&#39;].unique():
    state_predictor_vec = extract_predictor_vec(state)
    if state == &#39;California&#39;:
        ca_predictors = state_predictor_vec
    else:
        control_predictors += [state_predictor_vec]

control_predictors = np.hstack(control_predictors)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;construction-of-synthetic-control&#34;&gt;Construction of synthetic control&lt;/h4&gt;
&lt;p&gt;And here is how we optimize the objective function.&lt;/p&gt;
&lt;p&gt;Note that in the original paper, the authors suggested to add feature importance weights $V$ to be tuned simultaneously with the control weights $W$. I am not convinced that these are required, especially if all features are scaled properly. But in order to reproduce the same results as in the paper, these importance weights are tuned here too. Without them, the optimization would have been a single line of least squares..&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def w_mse(w, v, x0, x1): return mean_squared_error(x1, x0.dot(w), sample_weight=v)

def w_constraint(w, v, x0, x1): return np.sum(w) - 1

def v_constraint(V, W, X0, X1, Z0, Z1): return np.sum(V) - 1

def fun_w(w, v, x0, x1): return fmin_slsqp(w_mse, w, bounds=[(0.0, 1.0)]*len(w), f_eqcons=w_constraint, 
                                           args=(v, x0, x1), disp=False, full_output=True)[0]

def fun_v(v, w, x0, x1, z0, z1): return mean_squared_error(z1, z0.dot(fun_w(w, v, x0, x1)))

def solve_synthetic_control(X0, X1, Z0, Z1, Y0):
    k,j = X0.shape
    V0 = 1/k*np.ones(k)
    W0 = 1/j*np.zeros(j).transpose()
    V = fmin_slsqp(fun_v, V0, args=(W0, X0, X1, Z0, Z1), bounds=[(0.0, 1.0)]*len(V0), disp=True, f_eqcons=v_constraint, acc=1e-6)
    W = fun_w(W0, V, X0, X1)
    return V, W

V, W = solve_synthetic_control(control_predictors, ca_predictors, control_outcomes_pre, ca_outcomes_pre, control_outcomes_post)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Optimization terminated successfully    (Exit mode 0)
            Current function value: 23.45850495358178
            Iterations: 5
            Function evaluations: 8
            Gradient evaluations: 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optimization terminated successfully, but we need to examine the obtained fit with respect to the features. The values for the &amp;lsquo;average state&amp;rsquo; are shown for reference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_predictors = X0.mean(axis=1)
print(&amp;quot;Table 1: Cigarette sales predictor means \n&amp;quot;)
display(pd.DataFrame(np.hstack([X1, X0.dot(W).reshape(-1,1), mean_predictors.reshape(-1,1)]), 
             columns=[&#39;Real California&#39;, &#39;Synthetic California&#39;, &#39;Average of 38 Controls&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Table 1: Cigarette sales predictor means 
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Real California&lt;/th&gt;
      &lt;th&gt;Synthetic California&lt;/th&gt;
      &lt;th&gt;Average of 38 Controls&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.076559&lt;/td&gt;
      &lt;td&gt;10.076559&lt;/td&gt;
      &lt;td&gt;10.076559&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;17.353238&lt;/td&gt;
      &lt;td&gt;17.353238&lt;/td&gt;
      &lt;td&gt;17.353238&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;89.422223&lt;/td&gt;
      &lt;td&gt;89.422223&lt;/td&gt;
      &lt;td&gt;89.422223&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;24.280000&lt;/td&gt;
      &lt;td&gt;24.280000&lt;/td&gt;
      &lt;td&gt;24.280000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;90.100000&lt;/td&gt;
      &lt;td&gt;90.099950&lt;/td&gt;
      &lt;td&gt;113.823684&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;120.200000&lt;/td&gt;
      &lt;td&gt;120.200020&lt;/td&gt;
      &lt;td&gt;138.089474&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;127.100000&lt;/td&gt;
      &lt;td&gt;127.100214&lt;/td&gt;
      &lt;td&gt;136.931579&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Seems perfect.&lt;/p&gt;
&lt;p&gt;We can inspect which of the states contributed most to the synthetic control construction. These states are interpreted to be similar to California. And because of the simplex constraint, the solution is sparse - there are just a few states with non-negative weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;largest_weights = list(df_outcome[&#39;LocationDesc&#39;].values[1+np.flip(W.argsort())][:5])
print(&#39;, &#39;.join(largest_weights))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Utah, North Carolina, North Dakota, Montana, New Hampshire
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now Let&amp;rsquo;s construct and examine the obtained synthetic control:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;SC_outcomes = np.vstack([Z0, Y0]).dot(W)
CA_outcomes = np.vstack([Z1, Y1]).flatten()
fig = plt.figure(figsize=(7.5,4.5)) 

plt.plot(range(START_TIME,STOP_TIME),SC_outcomes, &#39;r--&#39;, label=&amp;quot;Synthetic California&amp;quot;);
plt.plot(range(START_TIME,STOP_TIME),CA_outcomes, &#39;b-&#39;, label=&amp;quot;California&amp;quot;);

plt.ylabel(&#39;per-capita cigarette sales (in packs)&#39;)
plt.xlabel(&#39;year&#39;)
plt.legend(loc=&#39;upper right&#39;)
plt.title(&amp;quot;Figure 2: Trends in per-capita cigarette sales: California vs. synthetic California&amp;quot;)
plt.axvline(INTERVENTION_TIME)
plt.text(x=INTERVENTION_TIME+0.2, y=30, s=&#39;Passage of Proposition 99&#39;)
plt.xlim([START_TIME, STOP_TIME-1])
plt.ylim([0, 140])
plt.grid()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/intro_files/output_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The pre-intervention fit is reasonable, so we can move on with confidence to calculate the causal effect. This is as simple as subtracting the two lines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Gap_outcome = np.vstack([Z0, Y0]).dot(W) - np.vstack([Z1, Y1]).flatten()
fig = plt.figure(figsize=(7.5,4.5)) 

plt.plot(range(START_TIME,STOP_TIME),Gap_outcome, &#39;r--&#39;);
plt.ylabel(&#39;gap in per-capita cigarette sales (in packs)&#39;)
plt.xlabel(&#39;year&#39;)
plt.title(&amp;quot;Figure 3: Per-capita cigarette sales gap between California and synthetic California&amp;quot;)
plt.axhline(0)
plt.axvline(INTERVENTION_TIME)
plt.text(x=INTERVENTION_TIME-9.5, y=30, s=&#39;Passage of Proposition 99&#39;)
plt.xlim([START_TIME, STOP_TIME-1])
plt.ylim([35, -35])
plt.grid()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/intro_files/output_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;With this plot, we can carefully conclude that the implementation of proposition 99 has caused a per-capita reduction of around 26 packs of cigarettes each year. Not bad!&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;p&gt;This post introduced the synthetic control method for policy evaluation and demonstrated its use on a famous example. I think this is a very interesting and fun topic that does not receive the attention it deserves. It is widely applicable and requires minimal and reasonable assumptions. The next post will go through another example that has not been addressed yet - what is the effect of Haifa&amp;rsquo;s newly introduced low emission zone (LEZ)? Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Air Pollution in Haifa</title>
      <link>https://tom-beer.github.io/post/air-pollution-haifa/</link>
      <pubDate>Mon, 15 Jun 2020 09:25:50 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/air-pollution-haifa/</guid>
      <description>&lt;p&gt;As part of a project evaluating the effect of Haifa&amp;rsquo;s low emission zone (LEZ) using the synthetic control method (coming soon!), I ran a short exploration of the data. It&amp;rsquo;s always beneficial to have a thorough look at the data before starting to model it. I think it&amp;rsquo;s safe to say that any investment of time in exploratory data analysis will be returned at a later stage in terms of better modelling insights, less bugs and valid conclusions.&lt;/p&gt;
&lt;p&gt;Disclaimer: I have no expertise in air quality analysis, in fact this is the first time I look at data collected from an air pollution monitoring system.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This post was generated from a Jupyter notebook, which can be downloaded from 
&lt;a href=&#34;https://github.com/tom-beer/Air-Pollution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The notebook goes through the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dataset cleaning and tidying&lt;/li&gt;
&lt;li&gt;Handling of missing values&lt;/li&gt;
&lt;li&gt;Analyzing trends of time series (daily, weekly, annual etc)&lt;/li&gt;
&lt;li&gt;Finding correlations between pairs of variables&lt;/li&gt;
&lt;li&gt;Data fusion with meteorological dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At a later stage, these sections will be added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple air pollution modelling&lt;/li&gt;
&lt;li&gt;Validation of the inference conducted for the ministry regarding the effect of Haifa&amp;rsquo;s LEZ.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataset-sources&#34;&gt;Dataset sources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Air quality monitoring data was downloaded from the website of Israel&amp;rsquo;s Ministry of Environmental Protection. The interface can be found 
&lt;a href=&#34;https://www.svivaaqm.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Temperature data was downloaded from Israel Meteorological Service&amp;rsquo;s website. The interface can be found 
&lt;a href=&#34;https://ims.data.gov.il/ims/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The raw data looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm = pd.read_csv(&#39;Data/Haifa-Atzmaut.csv&#39;)
df_aqm
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;תחנה:עצמאות חיפה -&lt;/th&gt;
      &lt;th&gt;SO2&lt;/th&gt;
      &lt;th&gt;Benzene&lt;/th&gt;
      &lt;th&gt;PM2.5&lt;/th&gt;
      &lt;th&gt;PM10&lt;/th&gt;
      &lt;th&gt;CO&lt;/th&gt;
      &lt;th&gt;NO2&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;NO&lt;/th&gt;
      &lt;th&gt;תאריך  \ שעה&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;ppb&lt;/td&gt;
      &lt;td&gt;ppb&lt;/td&gt;
      &lt;td&gt;µg/m³&lt;/td&gt;
      &lt;td&gt;µg/m³&lt;/td&gt;
      &lt;td&gt;ppm&lt;/td&gt;
      &lt;td&gt;ppb&lt;/td&gt;
      &lt;td&gt;ppb&lt;/td&gt;
      &lt;td&gt;ppb&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;29.7&lt;/td&gt;
      &lt;td&gt;44.4&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;41.2&lt;/td&gt;
      &lt;td&gt;114.8&lt;/td&gt;
      &lt;td&gt;73.8&lt;/td&gt;
      &lt;td&gt;31/12/2011 24:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;30.6&lt;/td&gt;
      &lt;td&gt;45.5&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;35.6&lt;/td&gt;
      &lt;td&gt;81.1&lt;/td&gt;
      &lt;td&gt;45.6&lt;/td&gt;
      &lt;td&gt;01/01/2012 00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;30.6&lt;/td&gt;
      &lt;td&gt;45.5&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;40.7&lt;/td&gt;
      &lt;td&gt;107.9&lt;/td&gt;
      &lt;td&gt;67.0&lt;/td&gt;
      &lt;td&gt;01/01/2012 00:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;30.2&lt;/td&gt;
      &lt;td&gt;45.1&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;41.7&lt;/td&gt;
      &lt;td&gt;127.9&lt;/td&gt;
      &lt;td&gt;86.2&lt;/td&gt;
      &lt;td&gt;01/01/2012 00:15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;883289&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;23/04/2020 02:15&lt;/td&gt;
      &lt;td&gt;31/03/2014 22:00&lt;/td&gt;
      &lt;td&gt;02/03/2014 17:00&lt;/td&gt;
      &lt;td&gt;07/02/2012 14:30&lt;/td&gt;
      &lt;td&gt;06/01/2020 13:40&lt;/td&gt;
      &lt;td&gt;18/01/2017 13:05&lt;/td&gt;
      &lt;td&gt;22/01/2015 07:30&lt;/td&gt;
      &lt;td&gt;22/01/2015 07:30&lt;/td&gt;
      &lt;td&gt;תאריך מקסימלי&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;883290&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;1.7&lt;/td&gt;
      &lt;td&gt;0.24&lt;/td&gt;
      &lt;td&gt;19.8&lt;/td&gt;
      &lt;td&gt;49.7&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
      &lt;td&gt;20.8&lt;/td&gt;
      &lt;td&gt;42.6&lt;/td&gt;
      &lt;td&gt;21.8&lt;/td&gt;
      &lt;td&gt;Avg&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;883291&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;237295&lt;/td&gt;
      &lt;td&gt;602180&lt;/td&gt;
      &lt;td&gt;800202&lt;/td&gt;
      &lt;td&gt;97432&lt;/td&gt;
      &lt;td&gt;844796&lt;/td&gt;
      &lt;td&gt;827176&lt;/td&gt;
      &lt;td&gt;826981&lt;/td&gt;
      &lt;td&gt;826945&lt;/td&gt;
      &lt;td&gt;כמות ערכים&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;883292&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
      &lt;td&gt;נתון[%]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;883293&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;2.8&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;17.8&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;16.3&lt;/td&gt;
      &lt;td&gt;58.6&lt;/td&gt;
      &lt;td&gt;46.0&lt;/td&gt;
      &lt;td&gt;סטיית תקן&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;883294 rows × 10 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;dataset-cleaning&#34;&gt;Dataset Cleaning&lt;/h3&gt;
&lt;p&gt;The following steps are needed to clean the dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Delete the first column, which contains only the station&amp;rsquo;s name&lt;/li&gt;
&lt;li&gt;Strip the column names of leading and trailing whitespaces&lt;/li&gt;
&lt;li&gt;Translate the Hebrew date/time column&lt;/li&gt;
&lt;li&gt;Replace the representation of midnight from 24:00 to 00:00 to comply with Python&amp;rsquo;s DateTime&lt;/li&gt;
&lt;li&gt;Drop the first row, which contains the units of measurements for the variables&lt;/li&gt;
&lt;li&gt;Drop the last 8 rows, which contain summary statistics&lt;/li&gt;
&lt;li&gt;Numerify the measurements, setting missing values to NaN&lt;/li&gt;
&lt;li&gt;Set the datetime variable as index&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;station_name = df_aqm.columns[0]
df_aqm.drop(columns=[station_name], inplace=True)
df_aqm.rename(columns={colname: colname.strip() for colname in df_aqm.columns}, inplace=True)
df_aqm.rename(columns={df_aqm.columns[-1]: &#39;DateTime&#39;}, inplace=True)
df_aqm[&#39;DateTime&#39;] = df_aqm[&#39;DateTime&#39;].apply(lambda x: x.strip().replace(&#39;24:00&#39;, &#39;00:00&#39;))
pollutants = list(df_aqm)
pollutants.remove(&#39;DateTime&#39;)
units = {colname: df_aqm.loc[0, colname].strip() for colname in pollutants}
df_aqm = df_aqm.iloc[1:-8]
df_aqm[pollutants] = df_aqm[pollutants].apply(pd.to_numeric, errors=&#39;coerce&#39;)
df_aqm[&#39;DateTime&#39;] = pd.to_datetime(df_aqm[&#39;DateTime&#39;],infer_datetime_format=True)
df_aqm = df_aqm.set_index(&#39;DateTime&#39;)

print(f&#39;The dataset spans {len(np.unique(df_aqm.index.date))} days,&#39; 
      f&#39;from {df_aqm.index[0].date()} to {df_aqm.index[-1].date()}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The dataset spans 3068 days,from 2011-12-31 to 2020-05-24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at a 2-month slice of the cleaned dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm.loc[&amp;quot;2018-04&amp;quot;:&amp;quot;2018-05&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;SO2&lt;/th&gt;
      &lt;th&gt;Benzene&lt;/th&gt;
      &lt;th&gt;PM2.5&lt;/th&gt;
      &lt;th&gt;PM10&lt;/th&gt;
      &lt;th&gt;CO&lt;/th&gt;
      &lt;th&gt;NO2&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;NO&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;DateTime&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-04-01 00:05:00&lt;/th&gt;
      &lt;td&gt;1.8&lt;/td&gt;
      &lt;td&gt;0.18&lt;/td&gt;
      &lt;td&gt;7.4&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;22.2&lt;/td&gt;
      &lt;td&gt;23.6&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-04-01 00:10:00&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.18&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;26.3&lt;/td&gt;
      &lt;td&gt;27.9&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-04-01 00:15:00&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.18&lt;/td&gt;
      &lt;td&gt;13.7&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;24.6&lt;/td&gt;
      &lt;td&gt;25.2&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-04-01 00:20:00&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.18&lt;/td&gt;
      &lt;td&gt;16.8&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;25.9&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-04-01 00:25:00&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.18&lt;/td&gt;
      &lt;td&gt;17.4&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
      &lt;td&gt;24.2&lt;/td&gt;
      &lt;td&gt;24.7&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-05-31 23:40:00&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.08&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;7.2&lt;/td&gt;
      &lt;td&gt;7.5&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-05-31 23:45:00&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.08&lt;/td&gt;
      &lt;td&gt;7.5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;7.1&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-05-31 23:50:00&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.08&lt;/td&gt;
      &lt;td&gt;6.4&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;10.2&lt;/td&gt;
      &lt;td&gt;11.2&lt;/td&gt;
      &lt;td&gt;1.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-05-31 23:55:00&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.08&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;9.1&lt;/td&gt;
      &lt;td&gt;10.1&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-05-31 00:00:00&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.08&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;7.7&lt;/td&gt;
      &lt;td&gt;7.4&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;17568 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Not bad! Nice and tidy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Some notes about the data&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Measurements are recorded at 5-minute intervals. That&amp;rsquo;s a very high resolution, much more than needed to answer the research question. But it&amp;rsquo;ll be very helpful - it will allow for aggressive smoothing of the noisy measurements. In total, we have almost 1 million measurements of each pollutant.&lt;/li&gt;
&lt;li&gt;From the slice above we can suspect that the measurement resolution for CO (Carbon Monoxide) might be small compared to its distribution. It&amp;rsquo;s easy to check:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm[&#39;CO&#39;].value_counts(normalize=True).head(7)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.3    0.338305
0.2    0.186482
0.4    0.182812
0.5    0.073091
0.1    0.051982
0.6    0.039140
0.0    0.035468
Name: CO, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And indeed more than 70% of the CO measurements fall in one of 3 distinct values. This implies that the CO measurement is not accurate enough in capturing this pollutant&amp;rsquo;s distribution. We should keep that in mind when drawing inferences based on CO.&lt;/p&gt;
&lt;p&gt;Moving on to a general overview of the data, let&amp;rsquo;s use the convenient &amp;lsquo;describe&amp;rsquo; method:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;SO2&lt;/th&gt;
      &lt;th&gt;Benzene&lt;/th&gt;
      &lt;th&gt;PM2.5&lt;/th&gt;
      &lt;th&gt;PM10&lt;/th&gt;
      &lt;th&gt;CO&lt;/th&gt;
      &lt;th&gt;NO2&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;NO&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;237295.000000&lt;/td&gt;
      &lt;td&gt;602180.000000&lt;/td&gt;
      &lt;td&gt;800202.000000&lt;/td&gt;
      &lt;td&gt;97432.000000&lt;/td&gt;
      &lt;td&gt;844796.000000&lt;/td&gt;
      &lt;td&gt;827176.000000&lt;/td&gt;
      &lt;td&gt;826981.000000&lt;/td&gt;
      &lt;td&gt;826945.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;1.718825&lt;/td&gt;
      &lt;td&gt;0.242009&lt;/td&gt;
      &lt;td&gt;19.774215&lt;/td&gt;
      &lt;td&gt;49.704957&lt;/td&gt;
      &lt;td&gt;0.366657&lt;/td&gt;
      &lt;td&gt;20.821325&lt;/td&gt;
      &lt;td&gt;42.579953&lt;/td&gt;
      &lt;td&gt;21.816400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;2.763984&lt;/td&gt;
      &lt;td&gt;0.300786&lt;/td&gt;
      &lt;td&gt;17.774355&lt;/td&gt;
      &lt;td&gt;49.961126&lt;/td&gt;
      &lt;td&gt;0.249257&lt;/td&gt;
      &lt;td&gt;16.310098&lt;/td&gt;
      &lt;td&gt;58.644492&lt;/td&gt;
      &lt;td&gt;46.070798&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;-0.800000&lt;/td&gt;
      &lt;td&gt;-0.010000&lt;/td&gt;
      &lt;td&gt;-29.500000&lt;/td&gt;
      &lt;td&gt;-10.100000&lt;/td&gt;
      &lt;td&gt;-0.400000&lt;/td&gt;
      &lt;td&gt;-4.800000&lt;/td&gt;
      &lt;td&gt;-4.600000&lt;/td&gt;
      &lt;td&gt;-5.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;0.100000&lt;/td&gt;
      &lt;td&gt;0.060000&lt;/td&gt;
      &lt;td&gt;10.800000&lt;/td&gt;
      &lt;td&gt;24.000000&lt;/td&gt;
      &lt;td&gt;0.200000&lt;/td&gt;
      &lt;td&gt;7.700000&lt;/td&gt;
      &lt;td&gt;8.500000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;0.800000&lt;/td&gt;
      &lt;td&gt;0.150000&lt;/td&gt;
      &lt;td&gt;17.200000&lt;/td&gt;
      &lt;td&gt;37.500000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
      &lt;td&gt;15.400000&lt;/td&gt;
      &lt;td&gt;19.300000&lt;/td&gt;
      &lt;td&gt;3.300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;2.400000&lt;/td&gt;
      &lt;td&gt;0.320000&lt;/td&gt;
      &lt;td&gt;25.400000&lt;/td&gt;
      &lt;td&gt;59.400000&lt;/td&gt;
      &lt;td&gt;0.400000&lt;/td&gt;
      &lt;td&gt;31.900000&lt;/td&gt;
      &lt;td&gt;51.300000&lt;/td&gt;
      &lt;td&gt;18.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;136.200000&lt;/td&gt;
      &lt;td&gt;12.700000&lt;/td&gt;
      &lt;td&gt;562.000000&lt;/td&gt;
      &lt;td&gt;810.900000&lt;/td&gt;
      &lt;td&gt;23.400000&lt;/td&gt;
      &lt;td&gt;262.400000&lt;/td&gt;
      &lt;td&gt;865.500000&lt;/td&gt;
      &lt;td&gt;763.800000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can immediately notice negative measurements for all pollutants. This is not a good sign, because while this can be easily addressed and fixed, it increases our suspicion of more serious problems in the dataset that would be difficult to identify.&lt;/p&gt;
&lt;p&gt;But that will be addressed later. Now we have to solve the issue of negative measurements. Let&amp;rsquo;s first understand the extent of the problem:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;percent_negative = 100*(df_aqm[pollutants] &amp;lt; 0).mean(axis=0).values
print(f&#39;{pollutants[percent_negative.argmax()]} has {percent_negative.max():.1f}% negative values&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NO has 8.8% negative values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s quite bad. Two of the simplest ways to fix this issue are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clip the negative values to 0&lt;/li&gt;
&lt;li&gt;Replace them with NaNs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, these two approaches may or may not be the best thing to do, depending on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mechanism that has caused this issue&lt;/li&gt;
&lt;li&gt;The quality of data required to answer the research question&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since I don&amp;rsquo;t have a substantiated answer to any of these questions, I tried to find out what domain practitioners recommend on doing. 
&lt;a href=&#34;https://www.researchgate.net/post/How_can_I_deal_with_negative_and_zero_concentrations_of_PM25_PM10_and_gas_data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; is a thread with some opinions and insights on the matter. The authors propose to differentiate between two situations, and act accordingly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the datapoints exhibit sudden short drops below 0, this probably indicates an instrument malfunction. In this case, it is advised to replace the invalid value with an average of some neighborhood.&lt;/li&gt;
&lt;li&gt;If consecutive values below 0 are recorded for long times compared to the instrument&amp;rsquo;s integration time, this is probably rooted in the true air conditions. In this case, negative values imply that the real value was below the instrument&amp;rsquo;s detection limit. The suggested approach would be to replace the negative values with the known detection limit, or 0 if it isn&amp;rsquo;t known (like in our case). More justifications for this approach can be found in 
&lt;a href=&#34;https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/98JD01212&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Polissar et al, 1998&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we want to follow these guidelines, we need to decide for each negative measurement if it the former case or the latter. This will be coded at a later stage.&lt;/p&gt;
&lt;h3 id=&#34;checking-for-systematic-data-collection-flaws&#34;&gt;Checking for systematic data collection flaws&lt;/h3&gt;
&lt;p&gt;It is important to make sure that our dataset does not have any systematic flaws in the data collection process.
For example, later we would like to demonstrate the fluctuations of pollutants as function of the weekday.
In this case, if some days are recorded significantly less often than other days, any conclusion we would like to state as for the weekday trend might be invalidated.&lt;/p&gt;
&lt;p&gt;An easy way to check this is to print the value counts of the weekdays:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm[&#39;Weekday Name&#39;] = df_aqm.index.day_name()
df_aqm[&#39;Weekday Name&#39;].value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sunday       126420
Saturday     126145
Friday       126144
Monday       126144
Wednesday    126144
Thursday     126144
Tuesday      126144
Name: Weekday Name, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the dataset is pretty much balanced in terms of weekday logging, and that is reassuring.
However, it could also be that the logging system simply adds an entry for every 5-minute interval, while the measurement instruments malfunction with some systematic (i.e not at random) pattern. A complete examination would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm[pollutants].isna().groupby(df_aqm[&#39;Weekday Name&#39;]).mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;SO2&lt;/th&gt;
      &lt;th&gt;Benzene&lt;/th&gt;
      &lt;th&gt;PM2.5&lt;/th&gt;
      &lt;th&gt;PM10&lt;/th&gt;
      &lt;th&gt;CO&lt;/th&gt;
      &lt;th&gt;NO2&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;NO&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Weekday Name&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Friday&lt;/th&gt;
      &lt;td&gt;0.731030&lt;/td&gt;
      &lt;td&gt;0.271008&lt;/td&gt;
      &lt;td&gt;0.100235&lt;/td&gt;
      &lt;td&gt;0.892718&lt;/td&gt;
      &lt;td&gt;0.040160&lt;/td&gt;
      &lt;td&gt;0.061295&lt;/td&gt;
      &lt;td&gt;0.061430&lt;/td&gt;
      &lt;td&gt;0.061430&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Monday&lt;/th&gt;
      &lt;td&gt;0.731204&lt;/td&gt;
      &lt;td&gt;0.394097&lt;/td&gt;
      &lt;td&gt;0.094646&lt;/td&gt;
      &lt;td&gt;0.887097&lt;/td&gt;
      &lt;td&gt;0.046114&lt;/td&gt;
      &lt;td&gt;0.067835&lt;/td&gt;
      &lt;td&gt;0.068121&lt;/td&gt;
      &lt;td&gt;0.068398&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Saturday&lt;/th&gt;
      &lt;td&gt;0.730992&lt;/td&gt;
      &lt;td&gt;0.344128&lt;/td&gt;
      &lt;td&gt;0.100820&lt;/td&gt;
      &lt;td&gt;0.894621&lt;/td&gt;
      &lt;td&gt;0.043846&lt;/td&gt;
      &lt;td&gt;0.064632&lt;/td&gt;
      &lt;td&gt;0.064767&lt;/td&gt;
      &lt;td&gt;0.064767&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Sunday&lt;/th&gt;
      &lt;td&gt;0.734813&lt;/td&gt;
      &lt;td&gt;0.313724&lt;/td&gt;
      &lt;td&gt;0.100791&lt;/td&gt;
      &lt;td&gt;0.884710&lt;/td&gt;
      &lt;td&gt;0.046757&lt;/td&gt;
      &lt;td&gt;0.060141&lt;/td&gt;
      &lt;td&gt;0.060354&lt;/td&gt;
      &lt;td&gt;0.060354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Thursday&lt;/th&gt;
      &lt;td&gt;0.728350&lt;/td&gt;
      &lt;td&gt;0.297026&lt;/td&gt;
      &lt;td&gt;0.093805&lt;/td&gt;
      &lt;td&gt;0.892662&lt;/td&gt;
      &lt;td&gt;0.043442&lt;/td&gt;
      &lt;td&gt;0.062254&lt;/td&gt;
      &lt;td&gt;0.062571&lt;/td&gt;
      &lt;td&gt;0.062579&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Tuesday&lt;/th&gt;
      &lt;td&gt;0.733376&lt;/td&gt;
      &lt;td&gt;0.305429&lt;/td&gt;
      &lt;td&gt;0.084047&lt;/td&gt;
      &lt;td&gt;0.886312&lt;/td&gt;
      &lt;td&gt;0.043363&lt;/td&gt;
      &lt;td&gt;0.069159&lt;/td&gt;
      &lt;td&gt;0.069413&lt;/td&gt;
      &lt;td&gt;0.069413&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Wednesday&lt;/th&gt;
      &lt;td&gt;0.729674&lt;/td&gt;
      &lt;td&gt;0.302345&lt;/td&gt;
      &lt;td&gt;0.084071&lt;/td&gt;
      &lt;td&gt;0.889745&lt;/td&gt;
      &lt;td&gt;0.041334&lt;/td&gt;
      &lt;td&gt;0.059353&lt;/td&gt;
      &lt;td&gt;0.059559&lt;/td&gt;
      &lt;td&gt;0.059559&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We see that the fraction of missing data changes significantly from one pollutant to another. In addition, for each pollutant, the fraction of missing values changes per weekday, but for now it&amp;rsquo;s hard to say if these differences are consequential. Meanwhile, it&amp;rsquo;s good to keep this table in mind, and we might return to it depending on the analysis and inference that we would like to conduct.&lt;/p&gt;
&lt;p&gt;So we can conclude this step with the reassurance that the data does not exhibit major systematic data collection flaws, and we can move on in to the analysis. Let&amp;rsquo;s plot the data in its most raw form - just a scatterplot of every datapoint:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;axes=df_aqm[pollutants].plot(marker=&#39;.&#39;, alpha=0.5, linestyle=&#39;None&#39;, figsize=(11, 11), subplots=True, colormap=&#39;Dark2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can notice a few things from this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PM10 and SO2 are not continuously monitored along the 8 year period. Plots and analyses in the following will not ignore them, as we can still draw some conclusions using these limited measurements&lt;/li&gt;
&lt;li&gt;We can see the first hint of a seasonal trend: it seems like NOx and NO follow an annual trend. Because of the scatterplot&amp;rsquo;s density it could also be that only a few outliers paint a picture that looks like an annual trend, while in reality they do not contribute a significant amount to the total trend. Therefore a more detailed analysis is required to validate this claim&lt;/li&gt;
&lt;li&gt;It seems like NO2 and NOx are identical (or almost identical measurements). Reading about 
&lt;a href=&#34;https://en.wikipedia.org/wiki/NOx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NOx&lt;/a&gt; it seems rather amusing, as NOx is defined as the sum the nitrogen oxides NO and NO2. But it is the visualization that is misleading, and a closer look reveals that NOx is always higher than NO2.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s see if on average there is a real annual trend for some substances:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;log_interval = 5  # minutes
samples_per_week = int(60 / log_interval * 24 * 7)
window = samples_per_week*4
overlap = int(2*samples_per_week)
df_aqm.rolling(
    window=window, center=False, min_periods=samples_per_week).mean()[samples_per_week*2::overlap].plot(
    marker=&#39;.&#39;, alpha=0.5, linestyle=&#39;None&#39;, figsize=(11, 11), subplots=True, colormap=&#39;Dark2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This plot shows rolling averages of the measurements, with windows of 4 weeks and an overlap of 2 weeks. A similar result could be obtained using a simple groupby with a monthly frequency:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df_aqm.groupby(pd.Grouper(freq=&#39;M&#39;)).mean()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;But the overlapping windows add a degree of smoothing which is (arguably) preferable in this case.&lt;/p&gt;
&lt;p&gt;If you are disappointed too with pandas&#39; lack of an overlap parameter in the rolling function, you can contribute to the discussion 
&lt;a href=&#34;https://github.com/pandas-dev/pandas/issues/15354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, enough with the ranting. The above plot shows that all nitrogen oxide measurements (and also benzene to a lesser extent) follow a clear annual trend peaking at winter. It is still not clear what&amp;rsquo;s the cause of this modulation, and we will revisit it later in the analysis.&lt;/p&gt;
&lt;p&gt;The same trends could be demonstrated using a different visualization, plotting the 8 year daily averages of every pollutants.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm.groupby(df_aqm.index.dayofyear).mean().plot(marker=&#39;.&#39;, alpha=0.5, linestyle=&#39;None&#39;, figsize=(11, 11), subplots=True, colormap=&#39;Dark2&#39;);
plt.xlabel(&#39;Day of the Year&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The annual trend of NO, NO2, NOx and benzene is validated through this plot. In addition, we can notice a peculiar drop in CO values around day 240. A closer look reveals that this drop is at day 243. This day is August 31st, the last day of summer vacation for 1.5 million kids. Coincidence? I don&amp;rsquo;t know. What do you think?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s move now to a weekly analysis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ax = sns.boxplot(data=df_aqm, x=&#39;Weekday Name&#39;, y=&#39;NOX&#39;, 
                 order=[&#39;Sunday&#39;, &#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;], 
                 palette=sns.cubehelix_palette(7, start=.5, rot=-.75, dark=0.3, reverse=True)).set_ylim([0,70]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For NOx, there is a clear weekly trend in the amount of air pollution measured. Note that the outliers have been cropped out of the plot. For other pollutants, the trend is not as impressive:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ax = sns.boxplot(data=df_aqm, x=&#39;Weekday Name&#39;, y=&#39;CO&#39;, 
                 order=[&#39;Sunday&#39;, &#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;], 
                 palette=sns.cubehelix_palette(7, start=.5, rot=-.75, dark=0.3, reverse=True)).set_ylim([0,1]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;And we also see the discrete nature of the CO measurements, as noted earlier. Another way to look at the weekly flactations would be to plot the raw measurements for a specific period of time:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_sub = df_aqm.loc[&#39;2019-07&#39;:&#39;2019-09&#39;] 
fig, ax = plt.subplots(figsize=(20,5))
ax.plot(df_sub.groupby(df_sub.index.date).median()[&#39;NOX&#39;], marker=&#39;o&#39;, linestyle=&#39;-&#39;)
ax.set_ylabel(&#39;NOx [ppb]&#39;)
ax.set_title(&#39;&#39;)
ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.SATURDAY))
ax.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this 3-month period, the lowest measured values were always on Saturdays, agreeing with the boxplot above.&lt;/p&gt;
&lt;p&gt;The third and last seasonal trend to be analyzed is the dependence on the hour of the day.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_aqm.groupby(df_aqm.index.time).mean().plot(marker=&#39;.&#39;, alpha=0.5, linestyle=&#39;None&#39;, figsize=(11, 11), subplots=True, colormap=&#39;Dark2&#39;);
plt.xticks([&amp;quot;08:00&amp;quot;,&amp;quot;12:00&amp;quot;, &amp;quot;16:00&amp;quot;, &amp;quot;20:00&amp;quot;, &amp;quot;00:00&amp;quot;, &amp;quot;04:00&amp;quot;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Some notes about this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most pollutants peak in the morning. Some right at the rush hour (NO, NO2, NOX, Benzene) and some a bit later (SO2, PM10)&lt;/li&gt;
&lt;li&gt;It doesn&amp;rsquo;t seem like PM2.5 adheres to any logical pattern.&lt;/li&gt;
&lt;li&gt;CO peaks around midnight. It seems as it has a slow buildup process, followed by a slow decreasing phase.&lt;/li&gt;
&lt;li&gt;If nitrogen oxides are caused mainly by transportation, then the rush hour attribution above could make sense. In this case, we could also attribute the slow increase and plateau in the afternoon/evening to the fact (or assumption?) that commute back home is spread on longer hours, compared to the narrow rush hour.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;m no expert on the sources of each pollutant, be it transportation, industry, the port or natural factors. So I don&amp;rsquo;t know if any of these hypotheses make any sort of sense.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;air-pollution-and-temperatures&#34;&gt;Air pollution and temperatures&lt;/h3&gt;
&lt;p&gt;The above trends show a clear correlation between the season/month and the average measured pollutants (at least for nitrogen oxides and benzene). But what is the cause of this modulation - is it the time itself? Or maybe it is the temperature? There could be many potential causes, and estimating each of these effects is a difficult problem, according to the identification strategies of causal inference. Instead, we can choose to stay in the realm of mere correlations, and while they do not imply causation, we could speculate what is a reasonable claim and what is not.&lt;/p&gt;
&lt;p&gt;A simple examination would be to evaluate how much of this modulation is governed by the change in temperature (disregarding the day of the year). To this end, I downloaded a second dataset, this time from Israeli Meteorological Service. It includes daily measurements (low and high temperatures) from the meteorological station closest to the air quality monitoring station, situated at BAZAN Group&amp;rsquo;s plant in Haifa.&lt;/p&gt;
&lt;p&gt;A straightforward cleaning yields a dataset that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_temp = pd.read_csv(&#39;Data/ims_data.csv&#39;, sep=&amp;quot;,&amp;quot;, encoding=&amp;quot;ISO-8859-8&amp;quot;)
# translate df from Hebrew
df_temp = df_temp.rename(columns={&#39;משתנה&#39;: &#39;variable&#39;, &#39;תאריך&#39;: &#39;DateTime&#39;, &#39;חיפה בתי זיקוק                                    (600)&#39;: &#39;value&#39;})
df_temp.loc[df_temp[&#39;variable&#39;] == &#39;טמפרטורת מינימום(C°)&#39;, &#39;variable&#39;] = &#39;min temp&#39;
df_temp.loc[df_temp[&#39;variable&#39;] == &#39;טמפרטורת מקסימום(C°)&#39;, &#39;variable&#39;] = &#39;max temp&#39;
df_temp.loc[df_temp[&#39;variable&#39;] == &#39;טמפרטורת מינימום ליד הקרקע(C°)&#39;, &#39;variable&#39;] = &#39;ground temp&#39;

# Delete the record &#39;ground temp&#39; (first make sure it&#39;s empty)
assert(df_temp.loc[df_temp[&#39;variable&#39;] == &#39;ground temp&#39;,&#39;value&#39;].values == &#39;-&#39;).all()
df_temp = df_temp.drop(df_temp[df_temp[&#39;variable&#39;] == &#39;ground temp&#39;].index)

# Convert to datetime
df_temp[&#39;DateTime&#39;] = pd.to_datetime(df_temp[&#39;DateTime&#39;],infer_datetime_format=True)

df_temp[&#39;value&#39;] = df_temp[&#39;value&#39;].apply(pd.to_numeric, errors=&#39;coerce&#39;)

df_temp.tail()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;variable&lt;/th&gt;
      &lt;th&gt;DateTime&lt;/th&gt;
      &lt;th&gt;value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;8638&lt;/th&gt;
      &lt;td&gt;min temp&lt;/td&gt;
      &lt;td&gt;2020-04-27&lt;/td&gt;
      &lt;td&gt;11.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8640&lt;/th&gt;
      &lt;td&gt;max temp&lt;/td&gt;
      &lt;td&gt;2020-04-28&lt;/td&gt;
      &lt;td&gt;23.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8641&lt;/th&gt;
      &lt;td&gt;min temp&lt;/td&gt;
      &lt;td&gt;2020-04-28&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8643&lt;/th&gt;
      &lt;td&gt;max temp&lt;/td&gt;
      &lt;td&gt;2020-04-29&lt;/td&gt;
      &lt;td&gt;22.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8644&lt;/th&gt;
      &lt;td&gt;min temp&lt;/td&gt;
      &lt;td&gt;2020-04-29&lt;/td&gt;
      &lt;td&gt;17.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s plot a histogram of the daily lows and highs, just to see that the dataset makes sense (it does):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;g = sns.FacetGrid(df_temp, hue=&#39;variable&#39;, height=6)
g.map(sns.distplot, &#39;value&#39;, bins=30, hist=True, kde=False, hist_kws={&amp;quot;linewidth&amp;quot;: 3, &amp;quot;alpha&amp;quot;: 0.7})
g.add_legend(title=&#39; &#39;)
g.axes[0,0].set_title(&amp;quot;Distribution of minimal and maximal temperatures&amp;quot;)
g.axes[0,0].set_xlabel(&#39;Temperature [Celsius]&#39;);

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To merge (fuse?) the datasets, we need to calculate an aggregate value for the pollutants for each day (because temperatures are measured only once a day). Averaging the measurements of each day is the simplest thing to do.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_pol_day = df_aqm.groupby(df_aqm.index.date).mean()
df_pol_day.index = pd.to_datetime(df_pol_day.index)

df_max = df_temp.loc[df_temp[&#39;variable&#39;] == &#39;max temp&#39;, [&#39;DateTime&#39;, &#39;value&#39;]]
df_max.rename(columns={&#39;value&#39;: &#39;max temp&#39;}, inplace=True)
df_max.set_index(&#39;DateTime&#39;, inplace=True, drop=True)

df_min = df_temp.loc[df_temp[&#39;variable&#39;] == &#39;min temp&#39;, [&#39;DateTime&#39;, &#39;value&#39;]]
df_min.rename(columns={&#39;value&#39;: &#39;min temp&#39;}, inplace=True)
df_min.set_index(&#39;DateTime&#39;, inplace=True, drop=True)

df_pol_temp = df_pol_day.merge(right=df_max, how=&#39;inner&#39;, left_index=True, right_index=True)
df_pol_temp = df_pol_temp.merge(right=df_min, how=&#39;inner&#39;, left_index=True, right_index=True)
df_pol_temp.tail()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;SO2&lt;/th&gt;
      &lt;th&gt;Benzene&lt;/th&gt;
      &lt;th&gt;PM2.5&lt;/th&gt;
      &lt;th&gt;PM10&lt;/th&gt;
      &lt;th&gt;CO&lt;/th&gt;
      &lt;th&gt;NO2&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;NO&lt;/th&gt;
      &lt;th&gt;max temp&lt;/th&gt;
      &lt;th&gt;min temp&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2020-04-25&lt;/th&gt;
      &lt;td&gt;0.031802&lt;/td&gt;
      &lt;td&gt;0.049213&lt;/td&gt;
      &lt;td&gt;10.824653&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.279152&lt;/td&gt;
      &lt;td&gt;1.761702&lt;/td&gt;
      &lt;td&gt;1.509220&lt;/td&gt;
      &lt;td&gt;0.010284&lt;/td&gt;
      &lt;td&gt;22.3&lt;/td&gt;
      &lt;td&gt;17.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2020-04-26&lt;/th&gt;
      &lt;td&gt;0.089753&lt;/td&gt;
      &lt;td&gt;0.036250&lt;/td&gt;
      &lt;td&gt;9.450694&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.295406&lt;/td&gt;
      &lt;td&gt;6.074823&lt;/td&gt;
      &lt;td&gt;7.617376&lt;/td&gt;
      &lt;td&gt;1.686170&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;16.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2020-04-27&lt;/th&gt;
      &lt;td&gt;0.217314&lt;/td&gt;
      &lt;td&gt;0.079306&lt;/td&gt;
      &lt;td&gt;12.359028&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.310247&lt;/td&gt;
      &lt;td&gt;18.504255&lt;/td&gt;
      &lt;td&gt;28.706383&lt;/td&gt;
      &lt;td&gt;10.386525&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;11.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2020-04-28&lt;/th&gt;
      &lt;td&gt;0.595760&lt;/td&gt;
      &lt;td&gt;0.091632&lt;/td&gt;
      &lt;td&gt;13.841319&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.332155&lt;/td&gt;
      &lt;td&gt;17.207801&lt;/td&gt;
      &lt;td&gt;26.422340&lt;/td&gt;
      &lt;td&gt;9.412057&lt;/td&gt;
      &lt;td&gt;23.2&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2020-04-29&lt;/th&gt;
      &lt;td&gt;0.102827&lt;/td&gt;
      &lt;td&gt;0.020035&lt;/td&gt;
      &lt;td&gt;10.708333&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.260777&lt;/td&gt;
      &lt;td&gt;1.787943&lt;/td&gt;
      &lt;td&gt;1.271986&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;22.9&lt;/td&gt;
      &lt;td&gt;17.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.jointplot(x=&#39;min temp&#39;, y=&#39;NOX&#39;, data=df_pol_temp, kind=&amp;quot;reg&amp;quot;, stat_func=corr_coef);
warnings.filterwarnings(action=&#39;default&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://tom-beer.github.io/Exploration_files/output_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;With a significant correlation of -0.58, the measured NOx values can be explained to some degree by the temperatures. It would be interesting to see if other meteorological factors, like humidity and wind, can explain even more of the NOx fluctuations.&lt;/p&gt;
&lt;h3 id=&#34;closing-remarks&#34;&gt;Closing remarks&lt;/h3&gt;
&lt;p&gt;This notebook described a preliminary exploration of the air quality monitoring data in Haifa. It is by no means an exhaustive analysis, it is just a set of steps used to increase the understanding of the dataset. If you have any comment - an analysis that you think should be done, a peculiar result that should be examined or anything else - I would love to know!&lt;/p&gt;
&lt;p&gt;This is not a conclusion, as it is only the start of the analysis. In the next chapter, I will evaluate the causal effect of Haifa&amp;rsquo;s new low emission zone using the synthetic control method. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Debiasing Doodle Polls</title>
      <link>https://tom-beer.github.io/post/debiasing-doodle-polls/</link>
      <pubDate>Thu, 02 Apr 2020 19:08:22 +0300</pubDate>
      <guid>https://tom-beer.github.io/post/debiasing-doodle-polls/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The full notebook and code can be found 
&lt;a href=&#34;https://github.com/tom-beer/Strategic-Doodle-Sim&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Did you know that Doodle polls are significantly affected by social bias? It turns out that open polls, where voters can see previous votes and their votes are visible to others, have different voting profiles compared to hidden polls. In a recent paper, 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt; show that voters act stratigically in open polls, and this change in behaviour includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open polls have higher response rates for very popular time slots&lt;/li&gt;
&lt;li&gt;Open polls have higher response rates for very unpopular time slots&lt;/li&gt;
&lt;li&gt;The average reported availability is higher in open polls compared to hidden polls&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first point not surprising - responders vote more for the popular time slots either because these alternatives are simply better for most voters, or because they feel the need to be cooperative with the group&amp;rsquo;s choices.
The second point however does not make sense at first - why would a repsonder vote for a time slot that is clearly not going to be selected? The authors suggest that this is because voters in open polls have an incentive to appear more cooperative:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;There is an implicit social expectation that every responder will mark as many slots as possible. Therefore a responder in open polls may be motivated to mark more slots. In other words, bearing in mind that other participants can see her name and vote, a participant may want to approve more time slots to appear more cooperative even if they are less convenient for herself.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The third point is likely a consequence of the first two points.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example of a typical Doodle poll. It is also available 
&lt;a href=&#34;https://doodle.com/poll/qx65aqr7ewmxrhrg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div&gt; &lt;img src=&#34;Images/poll-demo2.jpg&#34; width=&#34;500&#34;/&gt; &lt;/div&gt;
&lt;h2 id=&#34;whats-wrong-with-that&#34;&gt;What&amp;rsquo;s wrong with that?&lt;/h2&gt;
&lt;p&gt;One might think there&amp;rsquo;s nothing wrong that voters in open polls behave differently than in hidden polls. But actually this can have a substantial negative impact on the quality of the selected time: 
&lt;a href=&#34;#references&#34;&gt;Alrawi et al. (2016)&lt;/a&gt; argue via a theoretical welfare analysis that when voters are being more generous with their time, this can lead to inferior time slots being selected.&lt;/p&gt;
&lt;h2 id=&#34;what-can-be-done&#34;&gt;What can be done?&lt;/h2&gt;
&lt;p&gt;While we can&amp;rsquo;t change human nature, we can come up with suggestions for models and algorithms that would cancel out the social bias. Since hidden polls have a higher chance of maximizing social utility, these algorithms should approximate what voting profile would have been recorded if the poll had been hidden.
In this short project, a collaboration with Bar Eini-Porat, we propose a simple solution to this problem. This notebook desribes the data and simulation we rely on, details the design considerations we faced, and shows the results of our suggested approach.&lt;/p&gt;
&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;p&gt;The rest of this notebook is organized as follows.
First the dataset of Doodle polls is presented. Then the process of simulating synthetic open polls from real hidden polls is described, followed by detail about the debiasing model. The notebook concludes with results and a discussion on limitations and future directions.&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;The data used for this project is a small slice of the dataset that was introduced in 
&lt;a href=&#34;#references&#34;&gt;Reinecke et al. (2013)&lt;/a&gt;. The original dataset, provided by Doodle, consisted of 1,771,430 anonymized date/time polls that were selected at random from a time period in mid-2011. Like 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt;, we focus our analysis on polls with at least three participants, at least four time slots and only yes/no options. Unlike 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt;, we analyze Doodle polls from all around the world, and use only hidden polls for our analysis. The obtained dataset consists of 104 polls and has a median of 9 responders and 10 time slots.&lt;/p&gt;
&lt;h3 id=&#34;simulating-open-polls-from-hidden-polls&#34;&gt;Simulating open polls from hidden polls&lt;/h3&gt;
&lt;p&gt;The foundamental problem of causal inference is what makes the problem of evaluating a proposed solution so difficult. Meaning we never have access to the two parallel worlds of both an open poll and a hidden poll describing the same scheduling problem with the same group of people. Following 
&lt;a href=&#34;#references&#34;&gt;Alrawi et al. (2016)&lt;/a&gt;, we treat the hidden polls of the dataset as ground truth: In a hidden poll each participant casts a vote based solely on their true availability, without introducing social considerations. This is the reason we chose to use only the hidden polls of the dataset.&lt;/p&gt;
&lt;p&gt;From each hidden poll we simulate an open poll using the following approach.
We keep the number of participants and the number of time slots unchanged.
The first voter (usually the poll initiator) is not affected by social considerations, so her approved time slots are unaltered in the simulated open poll compared to the original hidden poll.
As for the next responders, their approved time slots may change according to the poll&amp;rsquo;s history up until their vote.&lt;/p&gt;
&lt;p&gt;In more detail, the simulation process consists of a three-step mechanism:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate individual utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An individual utility $u_{ij}$ with $0 \leq u_{ij} \leq 1$ is assinged to each voter $v_i$ for each time slot $a_j$ indicating her valuation of attending the meeting or event during that time slot. These utilities are called &amp;lsquo;individual&amp;rsquo; as they are not affected by other voters. We assume that there is a global, fixed &amp;lsquo;yes-threshold&amp;rsquo; $\tau_1$ that represents the utility beyond which a voter “typically” votes yes, so each voter $v_i$ is expected to say yes to a time slot $a_j$ when her utility for that slot satisfies $u_{ij} \geq \tau_1$. Then, in order to comply with the hidden poll, we generate individual utilities greater then $\tau_1$ for the approved votes in the original hidden poll, and smaller then $\tau_1$ for the disapproved votes:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$u^{ind}(a) \sim U[\tau_1,1] \text{ for } a \in A_1$$&lt;/p&gt;
&lt;p&gt;$$u^{ind}(a) \sim U[0,\tau_1] \text{ for } a \notin A_1$$&lt;/p&gt;
&lt;p&gt;Where $A_1$ is the set of approved time slots for a voter in the original hidden poll.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate social utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Like in the case of the individual utilities, social utilities are generated for each voter and time slot in each poll. The social utilities reflect the behavioral difference between hidden and open polls, i.e. they will be a function of the popularity of the time slot. Time slot popularity is defined as the ratio between approved votes to total number of votes.&lt;/p&gt;
&lt;p&gt;In addition, although it was not discussed in 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt;, we believe that the social utilities should also depend on the relative poll position, i.e. the fraction of the current voter to the total number of poll participants. For example, the second voter is less affected by social bias compared to the 10th voter. Since we do not have an accurate measure of the magnitude of the strategic behavior described in 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt;, we set this magnitude so that it will comply with the increase in average availability in open polls compared to hidden polls (from 0.53 to 0.39, see section 2.3 of 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Apart from the magnitude of the social effect, we also need to set a functional form for it. We propose a sigmoidal-shaped curve for the social utility as function of popularity. This is an asymmetrical gain-gain function that outputs an unpopularity-gain for values below some neutral popularity and a popularity gain for values above it. Similarly, we propose a sigmoidal-shaped curve for the social utility as function of relative position.&lt;/p&gt;
&lt;p&gt;The next figure illustrates these functions. The dashed vertical line denotes the neutral popularity, it need not be set at 0.5.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt; &lt;img src=&#34;Images/social_utility_funcs.JPG&#34; width=&#34;1000&#34;/&gt; &lt;/div&gt;
&lt;p&gt;The social utility as function of both popularity and relative position is the product of the above two functions, scaled down by a factor representing the maximal possible social utility.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Generate approved time slots&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, an open poll is deterministically simulated from the hidden poll and generated utilities in accordance with the social voting model of 
&lt;a href=&#34;#references&#34;&gt;Zou et al. (2015)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$\text{Approve  } { A_1^n,A_2^n \cap  Popular, A_2^n \cap  Unpopular }$$&lt;/p&gt;
&lt;p&gt;According to this model, there are three preference levels. A voter approves all of her most preferred slots, irrespective of their popularity. In addition, among the slots at her second preference level, the voter approves those slots that are either very popular or very unpopular. No slot of the third preference level is approved. $\tau_1$ is the decision boundry between $A_1$ and $A_2$, and $\tau_2$ is the decision boundry between $A_2$ and $A_3$.&lt;/p&gt;
&lt;p&gt;Therefore, approved votes in the synthetic open poll will be those exceeding the individual threshold (those approved in the original hidden poll), or those that, together with the social utilities, exceed the threshold:
\begin{align*}
a: u^{ind}(a) &amp;gt; {\tau_1} ; \cup ; a: u^{ind}(a)+u^{soc}(a) &amp;gt; {\tau_{1}}
\end{align*}&lt;/p&gt;
&lt;p&gt;An important note is that the magnitude of the social effect should not exceed the difference between the thresholds of the preference groups:
\begin{align*}
\text{Maximal Social Utility} \leq \tau_1+\tau_2
\end{align*}
This assures that time slots in the least preferred level ($A_3$) will not be approved, regardless of the social effect.&lt;/p&gt;
&lt;h3 id=&#34;debiasing-the-open-polls&#34;&gt;Debiasing the open polls&lt;/h3&gt;
&lt;p&gt;Now that we have simulated open polls, we can propose a method for circumventing the social effect. Our debiasing strategy is also a three-step process. First, we attempt to recover the social utilities that were in play for each voter and time slot. Then, we use these social utilities to weight the approved votes, with weight representing vote genuinity (a genuine vote is one that would have been approved irrespective of social considerations). Finally, we aggregate the weights to declare a winning time slot.&lt;/p&gt;
&lt;p&gt;In more detail:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Estimate social utilities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this step the social utility as function of popularity and relative position is estimated. We assume that both the functional form and social effect magnitude are known to us. This is a realistic scenario, as these quantities can be estimated efficiently from a large dataset.
Computationally, this is equivalent to the first step of the simulation process. However, in the simulation process the utilities are calculated based on the ground-truth hidden poll, and here the social utilities are estimated based on the open poll history, which is itself biased from the ground-truth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Calculate weights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this step, we weight the voters according to the probability that their votes are genuine. Our weighting mechanism is tightly related to the estimated social utilities. It is defined as follows, where $\hat{S_{ij}}$ are the estimated social utilities, $\alpha$ is a hyperparameter and $W_{ij}$ are the weights:
$$ W_{ij} = 1-\alpha \cdot \hat{S_{ij}} \text{ for } \alpha &amp;lt; 1 $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Declare winning time slot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final step is to declare a winning time slot, aggregating information from the calculated weights. We want to minimize the risk that we falsely disapprove a vote that was actually approved in the hidden poll. We can fallback to the Doodle default algorithm (labelled DDA) and choose the most popular time slot whenever we are not confident with our approach.&lt;/p&gt;
&lt;p&gt;We define the DDA score to be number of votes for each time slot. We define the debiased score to be the sum of weights for each time slot. The DDA winner and debiased winner are the maximal scoring time slot in each scoring regime.&lt;/p&gt;
&lt;p&gt;We choose to output a winner according to the reweighting mechanism only in cases with relatively high confidence: We recommend the highest scoring alternative in our model only in cases witg substantial score difference. Our final winning candidate for the open poll is defined as:&lt;/p&gt;
&lt;p&gt;$\text{if DDA score - debiased score} \geq \beta \cdot \text{debiased score:} $&lt;/p&gt;
&lt;p&gt;$\text{ final winner} = \text{DDA winner}$&lt;/p&gt;
&lt;p&gt;$\text{else final winner} = \text{debiased winner}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The proposed method&amp;rsquo;s performance is evaluated in the following manner. For each poll, there are four possible scenarios:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The winner in the original hidden poll matches both the winner proposed by DDA and the winner proposed by our method in the simulated open poll and.&lt;/li&gt;
&lt;li&gt;The winner in the original hidden poll does not match neither the winner proposed by DDA nor the winner proposed by our method in the simulated open poll and.&lt;/li&gt;
&lt;li&gt;The winner in the original hidden poll matches the winner proposed by DDA, but does not match the winner proposed by our method.&lt;/li&gt;
&lt;li&gt;The winner in the original hidden poll does not match the winner proposed by DDA, but matches the winner proposed by our method.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We count the number of polls in each of the above cases. We define the method&amp;rsquo;s benefit as the difference between the number of polls in case 4 and the number of polls in case 3. A positive benefit means that our method yields more good than harm.&lt;/p&gt;
&lt;p&gt;Following is a summary of obtained benefits as function of the number of voters and the number of time slots. It is apparent that the proposed method yields positive outcomes only in cases with a relatively high number of voters and time slots.&lt;/p&gt;
&lt;div&gt; &lt;img src=&#34;Images/errors.png&#34; width=&#34;500&#34;/&gt; &lt;/div&gt;
&lt;p&gt;If the method is restricted to these cases only, we obtain the following statistics:&lt;/p&gt;
&lt;div&gt; &lt;img src=&#34;Images/errors_constrained.png&#34; width=&#34;500&#34;/&gt; &lt;/div&gt;
Which means that it is possible to devise a new voting rule that has a strictly positive benefit.
&lt;h3 id=&#34;discussion-limitations-and-future-work&#34;&gt;Discussion, limitations and future work&lt;/h3&gt;
&lt;p&gt;This notebook demonstrated a simple proof of concept to circumvent some of the adverse effects of the social voting phenomenon. Of course, further discussion about the work&amp;rsquo;s limitations is warranted.&lt;/p&gt;
&lt;p&gt;First, these results have limited statistical validity, as they are constrained by the small dataset available to us. Even though each hidden poll was used to construct many open polls, the effective sample size is still limited to the number of original hidden polls.
To alleviate this obstacle, we have also implemented a completely synthetic simulation that does not require real hidden polls. But since these simulated hidden polls are of questionable quality compared to the real world distribution, we found it hard to draw inferences based on them instead of the real data.&lt;/p&gt;
&lt;p&gt;Second, the whole simulation and analysis environment lacks in external validity. The predictive ability of retrospective analyses is inherently problematic, and to make valid inferences, a true randomized controlled experiment would be required.&lt;/p&gt;
&lt;p&gt;To conclude, while further investigation is recommended, this work demonstrated the potential for a simple voting rule to increase overall social welfare.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Alrawi, D., Anthony, B. M., and Chung, C. (2016). &lt;strong&gt;How well do doodle polls do?&lt;/strong&gt; In Lecture Notes in Computer
Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume
10046 LNCS, pages 3–23. Springer Verlag.&lt;/p&gt;
&lt;p&gt;Reinecke, K., Nguyen, M. K., Bernstein, A., Naf, M., and Gajos, K. Z. (2013). &lt;strong&gt;Doodle around the world:
Online scheduling behavior reflects cultural differences in time perception and group decision-making.&lt;/strong&gt; In
Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW, pages 45–54.&lt;/p&gt;
&lt;p&gt;Zou, J., Meir, R., and Parkes, D. C. (2015). &lt;strong&gt;Strategic voting behavior in doodle polls.&lt;/strong&gt; In CSCW 2015 - Proceedings
of the 2015 ACM International Conference on Computer-Supported Cooperative Work and Social Computing, pages
464–472. Association for Computing Machinery, Inc.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
